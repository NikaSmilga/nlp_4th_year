{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "hw2_nlp_Smilga.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56684868",
        "outputId": "14fa9d92-b50a-41fe-9844-2e2870321af5"
      },
      "source": [
        "pip install langid"
      ],
      "id": "56684868",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langid in /usr/local/lib/python3.7/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from langid) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7SGbOe-VBlm",
        "outputId": "3817b150-5bcc-4b09-f7e5-10927fe933e3"
      },
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "import nltk\n",
        "import string\n",
        "from collections import Counter\n",
        "import langid\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()"
      ],
      "id": "k7SGbOe-VBlm",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3IN3lHtmpgp"
      },
      "source": [
        "1) Можно использовать bootstrapping в широком понимании. В нашем случае с отзывами это значило бы найти довольно большое количество шаблонов типа 'I just adore X', 'X is my favourite', 'I have used X for ...' и выделить из них то, что стоит на месте Х. Для этого способа нужно пройтись по корпусу (а он у нас очень большой!) и вручную выделить нужные шаблоны. Итак, данные: сам корпус + выделенные на его основании шаблоны.\n",
        "<br>Проблем с эти способом, кажется, больше, чем его преимуществ: \n",
        "<br>- не все пользователи четко и ясно пишут, что им нравится этот продукт/это их любимый продукт. Формулировок может быть практически бесконечное множество, собрать все (или почти все) возможные шаблоны займет очень много времени.\n",
        "<br>- проблемы с сущностями, состоящими из 2+ слов. Например, один пользователь напишет 'I like Aveda', другой 'I like Aveda smoother', третий 'I like Aveda Smooth Infusion Hair Freeze Smoother'. Кажется, что во всех этих случаях мы выделим только Aveda, потеряв остальные части сущности. Если пытаться выделить больше, то можно наоборот забрать лишние ненужные слова (трудно жить без морф. информации!)\n",
        "<br>- найденные сущности смешаются с найденные дескрипторами и отделить друг от друга не вручную будет малореально\n",
        "<br>+ процесс четко контролируемый, если мы увидим, что выдается что-то не то, можно будет отследить это и изменить/убрать соответствующий шаблон\n",
        "<br><br>2) Можно самим составить словарь некоторых сущностей/дескрипторов, основываясь на метаданных (названиях продуктов) и расширить его с помощью эмбеддингов (для дескрипторов можно использовать как обученные на корпусе, так и предобученные, для сущностей -- только обученные на нашем корпусе). Нам понадобится: корпус + метаданные + выделенные на основании метаданных дескрипторы и некоторые сущности\n",
        "<br>- скорее всего, мы потеряем редко встречающиеся сущности и сущности, использующиеся в необычных контекстах\n",
        "<br>- здесь мы по сути используем черный ящик: если нам выдастся что-то совсем неподходящее, мы вряд ли сможем понять, почему это произошло и устранить причину, придется подчищать результаты от всего мусора вручную\n",
        "<br>+ не нужно вручную искать и задавать все возможные контексты использования сущностей, как в первом варианте; будут учтены не только \"захардкоженные\" нами контексты, но и контексты, немного отличающиеся друг от друга \n",
        "<br>+ можно отдельно смотреть дескрипторы+синонимы(векторные) и сущности+синонимы(векторные), если нам это интересно (а это может быть полезно для работы с коллокациями, содержащими сущности)\n",
        "<br><br>3) Этот способ похож одновременно на первый и второй, но также использует морфологическую информацию. Можно составить список возможных дескрипторов, основываясь на метаданных, затем составить с ними шаблоны, использующиеся морфологическую информацию. yargy тут подойдет плохо, тк он не работает так с английским, но мы будем использовать библиотеку spaCy, работающую примерно также. В качестве сущностей будем выбирать имена собственные. Тут нам понадобится корпус + метаданные + выделенные на основе метаданных дескрипторы.\n",
        "<br>- список дескрипторов зависит только от нас и должен быть максимально полным: если мы упустим дескриптор, мы упустим все принадлежащие ему сущности\n",
        "<br>- теряем сущности, которые ни разу не попались рядом с дескриптором, хотя другими методами могли бы их найти\n",
        "<br>+ дескрипторы отдельно, именованные сущности отдельно\n",
        "<br>+ можем контролировать и корректировать шаблоны, не черный ящик\n",
        "<br>+ учитываем морф.информацию, будет меньше мусора\n",
        "<br>+ можем брать сколько угодно большие сущности, хоть из 6 слов, главное, чтоб они шли рядом с дескриптором\n",
        "<br>+ выделять надо только дескрипторы, а не дескрипторы и сущности, их не так много, тк у нашего корпуса четкая однородная тематика"
      ],
      "id": "h3IN3lHtmpgp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4qOKp8kUjtf"
      },
      "source": [
        "Этой функцией мы будем предобрабатывать текст, отделяя знаки препинания, приводя все к нижнему регистру и избавляясь от стоп слов."
      ],
      "id": "b4qOKp8kUjtf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQY7dcOZgcUi"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"([\\w/'+$\\s-]+|[^\\w/'+$\\s-]+)\\s*\", r\"\\1 \", text)\n",
        "    text = [x for x in text.split(' ') if x not in stopwords.words('english')]\n",
        "    return ' '.join(text)"
      ],
      "id": "EQY7dcOZgcUi",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydvEqGquVE76"
      },
      "source": [
        "Скачиваем данные, я беру датасеты с люксовой косметикой."
      ],
      "id": "ydvEqGquVE76"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJdLbEm_dLAp",
        "outputId": "67a265f2-bd2d-4860-ee10-5f44a7cfa68e"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Luxury_Beauty.json.gz\n",
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Luxury_Beauty.json.gz"
      ],
      "id": "vJdLbEm_dLAp",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-04 18:18:41--  http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Luxury_Beauty.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7608964 (7.3M) [application/octet-stream]\n",
            "Saving to: ‘meta_Luxury_Beauty.json.gz.1’\n",
            "\n",
            "meta_Luxury_Beauty. 100%[===================>]   7.26M  6.16MB/s    in 1.2s    \n",
            "\n",
            "2021-12-04 18:18:43 (6.16 MB/s) - ‘meta_Luxury_Beauty.json.gz.1’ saved [7608964/7608964]\n",
            "\n",
            "--2021-12-04 18:18:43--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Luxury_Beauty.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73193215 (70M) [application/octet-stream]\n",
            "Saving to: ‘Luxury_Beauty.json.gz.1’\n",
            "\n",
            "Luxury_Beauty.json. 100%[===================>]  69.80M  17.1MB/s    in 4.1s    \n",
            "\n",
            "2021-12-04 18:18:47 (17.1 MB/s) - ‘Luxury_Beauty.json.gz.1’ saved [73193215/73193215]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5090dfca",
        "scrolled": true
      },
      "source": [
        "data = []\n",
        "with gzip.open('meta_Luxury_Beauty.json.gz') as f:\n",
        "    for l in f:\n",
        "        data.append(json.loads(l.strip()))\n",
        "\n",
        "data1 = []\n",
        "with gzip.open('Luxury_Beauty.json.gz') as f:\n",
        "    for l in f:\n",
        "        data1.append(json.loads(l.strip()))"
      ],
      "id": "5090dfca",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXG0uZJd8R8"
      },
      "source": [
        "df = pd.DataFrame.from_dict(data1)\n",
        "df_meta = pd.DataFrame.from_dict(data)"
      ],
      "id": "nUXG0uZJd8R8",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb8NfURsVXNz"
      },
      "source": [
        "Я также скачала метаданные. К сожалению, я сделала первую часть дз до того, как узнала, что тут можно было извлекать сущности автоматически, поэтому я просто посмотрела на самые популярные биграммы и триграммы в заголовках, и на основании них сделала выводы о популярных дескрипторах. Закомменчу и скрою вывод с биграммами и триграммами, чтобы не засорять тетрадку."
      ],
      "id": "Kb8NfURsVXNz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a1edfef"
      },
      "source": [
        "titles = ' '.join(list(df_meta['title']))"
      ],
      "id": "2a1edfef",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94320ade"
      },
      "source": [
        "tokens = [x for x in nltk.word_tokenize(titles) if x not in string.punctuation]\n",
        "c = Counter(tokens).most_common()\n",
        "bgs = nltk.bigrams(tokens)\n",
        "fdist = nltk.FreqDist(bgs)\n",
        "freq_bi = []\n",
        "for k,v in fdist.items():\n",
        "    freq_bi.append([' '.join(k), v])\n",
        "freq_bi = sorted(freq_bi, key = lambda x: int(x[1]), reverse=True)\n",
        "tgs = nltk.trigrams(tokens)\n",
        "fdist = nltk.FreqDist(tgs)\n",
        "freq_tri = []\n",
        "for k,v in fdist.items():\n",
        "    freq_tri.append([' '.join(k), v])\n",
        "freq_tri = sorted(freq_tri, key = lambda x: int(x[1]), reverse=True)"
      ],
      "id": "94320ade",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a49d50d"
      },
      "source": [
        "#[x for x in freq_bi if 'fl' not in x[0] and 'oz' not in x[0] and 'Oz' not in x[0]]"
      ],
      "id": "4a49d50d",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15dafc82"
      },
      "source": [
        "#[x for x in freq_tri if 'fl' not in x[0] and 'oz' not in x[0] and 'Oz' not in x[0]]"
      ],
      "id": "15dafc82",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7Iz8_JV6zd"
      },
      "source": [
        "Немного предобработаю тексты отзывов, избавлюсь от совсем коротких, они явно нам не пригодятся (тк там не будет либо названия продукта, либо полезной информации о нем, а скорее всего и того и того). Разделю корпус отзывов на чанки по 1000 штук, чтобы было легче и удобнее их обрабатывать. МСами отзывы в корпусе разделю \\n."
      ],
      "id": "2f7Iz8_JV6zd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoaPq6-SqWga"
      },
      "source": [
        "df = df.dropna(subset=['reviewText'])\n",
        "reviews = list([str(x) for x in df['reviewText']])\n",
        "reviews = [x for x in reviews if len(list(x))>8]\n",
        "chunks_pre = [reviews[x:x+100] for x in range(0, len(reviews), 1000)]\n",
        "chunks = [' \\n '.join(x) for x in chunks_pre]\n",
        "chunks1 = [' /// '.join(x) for x in chunks_pre]"
      ],
      "id": "qoaPq6-SqWga",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PRdkPJYWrhC"
      },
      "source": [
        "Список полученных мной возможных дескрипторов. В первом списке самостоятельные дескрипторы, во втором и третьем -- дескрипторы, которые обычно идут в паре друг в другом (соотетственно 1-й и 2-й)."
      ],
      "id": "0PRdkPJYWrhC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtyvLigzatbv"
      },
      "source": [
        "descriptors = ['product',\n",
        "    'perfume',\n",
        "    'spray',\n",
        "    'cream',\n",
        "    'lotion',\n",
        "    'gloss',\n",
        "    'glosses',\n",
        "    'sunscreen',\n",
        "    'tan',\n",
        "    'moisturizer',\n",
        "    'moisturiser',\n",
        "    'scrub',\n",
        "    'mascara',\n",
        "    'powder',\n",
        "    'conditioner',\n",
        "    'shampoo',\n",
        "    'gel',\n",
        "    'cleanser',\n",
        "    'mask',\n",
        "    'treatment',\n",
        "    'brush',\n",
        "    'brushes',\n",
        "    'pencil',\n",
        "    'toothpaste',\n",
        "    'foundation',\n",
        "    'wax',\n",
        "    'waxes',\n",
        "    'balm',\n",
        "    'detangler',\n",
        "    'clay',\n",
        "    'bb'\n",
        "]\n",
        "\n",
        "first_descriptors = [\n",
        "    'nail',\n",
        "    'hand',\n",
        "    'eau',\n",
        "    'hair',\n",
        "    'lip',\n",
        "    'body',\n",
        "    'face',\n",
        "    'hair'\n",
        "]\n",
        "\n",
        "second_descriptors = [\n",
        "    'polish',\n",
        "    'polishes',\n",
        "    'cream',\n",
        "    'de',\n",
        "    'dryer',\n",
        "    'balm',\n",
        "    'oil',\n",
        "    'butter',\n",
        "    'wash',\n",
        "    'washes',\n",
        "    'lotion'\n",
        "]\n",
        "\n",
        "all_descs = descriptors + [x + 's' for x in descriptors] + first_descriptors + second_descriptors + ['parfum', 'toilette']"
      ],
      "id": "rtyvLigzatbv",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99L-e4qCXJAz"
      },
      "source": [
        "Поскольку yargy нормально работает только с русским языком, я вопсользуюсь аналогичной функцией spaCy. Буду искать идущие рядом с дескрипторами имена собственным (PROPN), возможно в комбинации с прилагательными, существительными и тд. При этом не учитываю меры измерения fl и oz, таким образом отсекая мусор. В процессе еще проверяю язык отзыва: из-за этого мы потеряем некоторые короткие отзывы, зато отсеется довольно много мусора на испанском, который spaCy идентифицировал на PROPN и который соответственно засорял наш список сущностей."
      ],
      "id": "99L-e4qCXJAz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "296a566d",
        "outputId": "73d7e563-6426-4c92-f9c1-d1ca667c14eb"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from tqdm import tqdm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab, validate=True)\n",
        "pattern = [[{\"LEMMA\": {\"IN\": descriptors}}, {\"POS\": \"NOUN\", \"OP\": \"?\"}, {\"LEMMA\": {\"NOT_IN\": ['fl', 'oz', 'FL', 'OZ']}, \"POS\": \"PROPN\", \"OP\": \"+\"}, \n",
        "            {\"POS\": {\"IN\": ['PUNCT', 'SYMB']}, \"OP\": \"?\"}, {\"LEMMA\": {\"NOT_IN\": ['fl', 'oz', 'FL', 'OZ']}, \"POS\": \"PROPN\", \"OP\": \"+\"}],\n",
        "           [{\"POS\": \"NOUN\", \"OP\": \"?\"}, {\"LEMMA\": {\"NOT_IN\": ['fl', 'oz', 'FL', 'OZ']}, \"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": {\"IN\": ['PUNCT', 'SYMB']}, \"OP\": \"?\"},\n",
        "            {\"LEMMA\": {\"NOT_IN\": ['fl', 'oz', 'FL', 'OZ']}, \"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"JJ\", \"OP\": \"*\"}, {\"LEMMA\": {\"IN\": descriptors}}],\n",
        "           [{\"POS\": \"NOUN\", \"OP\": \"?\"}, {\"LEMMA\": {\"NOT_IN\": ['fl', 'oz', 'FL', 'OZ']}, \"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": {\"IN\": ['PUNCT', 'SYMB']}, \"OP\": \"?\"},\n",
        "            {\"LEMMA\": {\"NOT_IN\": ['fl', 'oz', 'FL', 'OZ']}, \"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"JJ\", \"OP\": \"*\"}, {\"LEMMA\": {\"IN\": first_descriptors}},\n",
        "            {\"LEMMA\": {\"IN\": second_descriptors}}, {\"LEMMA\": {\"IN\": ['parfum', 'toilette']}, \"OP\": \"?\"}],\n",
        "           [{\"LEMMA\": {\"IN\": first_descriptors}}, {\"LEMMA\": {\"IN\": second_descriptors}}, {\"LEMMA\": {\"IN\": ['parfum', 'toilette']}, \"OP\": \"?\"},\n",
        "            {\"POS\": \"NOUN\", \"OP\": \"?\"}, {\"LEMMA\": {\"NOT_IN\": ['fl', 'oz', 'FL', 'OZ']}, \"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": {\"IN\": ['PUNCT', 'SYMB']}, \"OP\": \"?\"},\n",
        "            {\"LEMMA\": {\"NOT_IN\": ['fl', 'oz', 'FL', 'OZ']}, \"POS\": \"PROPN\", \"OP\": \"+\"}]]\n",
        "matcher.add(\"beautyproduct\", pattern)\n",
        "\n",
        "nes = []\n",
        "nes_desc = []\n",
        "for chunk in tqdm(chunks):\n",
        "  for sent in chunk.split('\\n'):\n",
        "    doc = nlp(sent)\n",
        "    matches = matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        string_id = nlp.vocab.strings[match_id]  \n",
        "        span = doc[start:end] \n",
        "        if langid.classify(sent)[0]=='en':\n",
        "          nes_desc.append(span.text)\n",
        "          no_desc = span.text.split()\n",
        "          for i in all_descs:\n",
        "            if i in no_desc:\n",
        "              no_desc.remove(i)\n",
        "          nes.append(' '.join(no_desc))"
      ],
      "id": "296a566d",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 558/558 [14:58<00:00,  1.61s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbBGMl7bYLK6"
      },
      "source": [
        "Я на всякий случай получаю два списка: ner, то есть сами найденные сущности без дескрипторов, и ner_desc, то есть найденные сущности + их дескрипторы."
      ],
      "id": "lbBGMl7bYLK6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VHc09jSCNLS"
      },
      "source": [
        "nes = [x for x in nes if len(x) > 1]"
      ],
      "id": "5VHc09jSCNLS",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niqZDuDPYqDt"
      },
      "source": [
        "Оставляю по 20 самых частотных сущностей. Кстати, распечатаем их, чтоб понять, адекватно ли у нас сработал поиск."
      ],
      "id": "niqZDuDPYqDt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT8_IbvzED6z"
      },
      "source": [
        "c = Counter()\n",
        "nes_desc_common = Counter(nes_desc).most_common(20)\n",
        "nes_common = Counter(nes).most_common(20)\n",
        "nes_set = set(nes)\n",
        "nes_desc_set = set(nes_desc)"
      ],
      "id": "tT8_IbvzED6z",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMpW2ORjYy44",
        "outputId": "ac3b1145-adff-4a49-bc4b-61a6ef2150bc"
      },
      "source": [
        "nes_desc_common"
      ],
      "id": "tMpW2ORjYy44",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mario Badescu products', 38),\n",
              " ('Jane Iredale products', 36),\n",
              " ('Paul Mitchell products', 24),\n",
              " ('Jack Black products', 20),\n",
              " ('Badger Hair brush', 16),\n",
              " ('Escali Badger Hair brush', 16),\n",
              " ('Juice Beauty products', 15),\n",
              " ('Hot Tools products', 12),\n",
              " ('Mason Pearson brush', 12),\n",
              " ('Paul Mitchell product', 8),\n",
              " ('Jane Iredale product', 7),\n",
              " ('Jack Black product', 7),\n",
              " ('Roche-Posay products', 7),\n",
              " ('Mason Pearson brushes', 6),\n",
              " ('La Roche-Posay products', 6),\n",
              " ('Anthony Logistics products', 5),\n",
              " ('Jack Black shampoo', 5),\n",
              " ('Bare Minerals powders', 5),\n",
              " ('Iredale BB cream', 4),\n",
              " ('Jane Iredale BB cream', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SM_MvMUY0rW",
        "outputId": "fbcaf215-2382-425f-e732-7f86ece14d2a"
      },
      "source": [
        "nes_common"
      ],
      "id": "-SM_MvMUY0rW",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jane Iredale', 47),\n",
              " ('Jack Black', 44),\n",
              " ('Mario Badescu', 44),\n",
              " ('Paul Mitchell', 41),\n",
              " ('Juice Beauty', 20),\n",
              " ('Mason Pearson', 18),\n",
              " ('Badger Hair', 16),\n",
              " ('Escali Badger Hair', 16),\n",
              " ('Hot Tools', 15),\n",
              " ('Bare Minerals', 12),\n",
              " ('Roche-Posay', 10),\n",
              " ('La Roche-Posay', 9),\n",
              " ('Billy Jealousy', 6),\n",
              " ('Anthony Logistics', 5),\n",
              " ('Iredale BB', 5),\n",
              " ('Jane Iredale BB', 5),\n",
              " ('Elizabeth Arden', 5),\n",
              " ('La Roche', 5),\n",
              " ('Molton Brown', 5),\n",
              " ('jane iredale', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxBofSpdY45X"
      },
      "source": [
        "Предобрабатываем полученные сущности и готовим их к поиску биграмм -- прилепляем друг к другу через _, чтоб они воспринимались программой как одно слово."
      ],
      "id": "OxBofSpdY45X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQwc-EKJ4NOX",
        "outputId": "3ad6c52d-9755-4064-9ee5-e0094261ec9e"
      },
      "source": [
        "nes_desc_prep = [preprocess_text(x[0]) for x in tqdm(nes_desc_common)]\n",
        "nes_prep = [preprocess_text(x[0]) for x in tqdm(nes_common)]\n",
        "nes_split = ['_'.join(x.split()) for x in nes_prep]\n",
        "nes_desc_split = ['_'.join(x.split()) for x in nes_desc_prep]"
      ],
      "id": "wQwc-EKJ4NOX",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 956.99it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 1159.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgZn_pQXZMic"
      },
      "source": [
        "Предобрабатываем тексты отзывов."
      ],
      "id": "GgZn_pQXZMic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYCZLQ51aIJe",
        "outputId": "0a246e3f-1092-485f-c44c-505a589a00b1"
      },
      "source": [
        "chunks_prep1 = [preprocess_text(c) for c in tqdm(chunks1)]"
      ],
      "id": "UYCZLQ51aIJe",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/558 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/558 [00:00<03:22,  2.76it/s]\u001b[A\n",
            "  0%|          | 2/558 [00:00<04:19,  2.14it/s]\u001b[A\n",
            "  1%|          | 3/558 [00:01<05:53,  1.57it/s]\u001b[A\n",
            "  1%|          | 4/558 [00:01<04:26,  2.08it/s]\u001b[A\n",
            "  1%|          | 5/558 [00:02<03:49,  2.41it/s]\u001b[A\n",
            "  1%|          | 6/558 [00:02<03:23,  2.71it/s]\u001b[A\n",
            "  1%|▏         | 7/558 [00:02<03:19,  2.76it/s]\u001b[A\n",
            "  1%|▏         | 8/558 [00:03<03:37,  2.53it/s]\u001b[A\n",
            "  2%|▏         | 9/558 [00:03<03:55,  2.33it/s]\u001b[A\n",
            "  2%|▏         | 10/558 [00:04<04:16,  2.14it/s]\u001b[A\n",
            "  2%|▏         | 11/558 [00:05<04:44,  1.92it/s]\u001b[A\n",
            "  2%|▏         | 12/558 [00:05<03:51,  2.36it/s]\u001b[A\n",
            "  2%|▏         | 13/558 [00:05<04:04,  2.23it/s]\u001b[A\n",
            "  3%|▎         | 14/558 [00:06<04:14,  2.14it/s]\u001b[A\n",
            "  3%|▎         | 15/558 [00:07<05:01,  1.80it/s]\u001b[A\n",
            "  3%|▎         | 16/558 [00:07<04:56,  1.83it/s]\u001b[A\n",
            "  3%|▎         | 17/558 [00:08<04:37,  1.95it/s]\u001b[A\n",
            "  3%|▎         | 18/558 [00:08<05:36,  1.61it/s]\u001b[A\n",
            "  3%|▎         | 19/558 [00:09<05:22,  1.67it/s]\u001b[A\n",
            "  4%|▎         | 20/558 [00:10<05:22,  1.67it/s]\u001b[A\n",
            "  4%|▍         | 21/558 [00:10<06:12,  1.44it/s]\u001b[A\n",
            "  4%|▍         | 22/558 [00:11<05:05,  1.76it/s]\u001b[A\n",
            "  4%|▍         | 23/558 [00:11<04:46,  1.87it/s]\u001b[A\n",
            "  4%|▍         | 24/558 [00:12<06:14,  1.43it/s]\u001b[A\n",
            "  4%|▍         | 25/558 [00:13<05:52,  1.51it/s]\u001b[A\n",
            "  5%|▍         | 26/558 [00:13<05:42,  1.55it/s]\u001b[A\n",
            "  5%|▍         | 27/558 [00:14<05:10,  1.71it/s]\u001b[A\n",
            "  5%|▌         | 28/558 [00:14<04:32,  1.95it/s]\u001b[A\n",
            "  5%|▌         | 29/558 [00:15<03:54,  2.26it/s]\u001b[A\n",
            "  5%|▌         | 30/558 [00:15<03:37,  2.43it/s]\u001b[A\n",
            "  6%|▌         | 31/558 [00:15<03:52,  2.27it/s]\u001b[A\n",
            "  6%|▌         | 32/558 [00:16<04:13,  2.08it/s]\u001b[A\n",
            "  6%|▌         | 33/558 [00:16<04:24,  1.99it/s]\u001b[A\n",
            "  6%|▌         | 34/558 [00:17<05:31,  1.58it/s]\u001b[A\n",
            "  6%|▋         | 35/558 [00:18<05:24,  1.61it/s]\u001b[A\n",
            "  6%|▋         | 36/558 [00:19<05:19,  1.63it/s]\u001b[A\n",
            "  7%|▋         | 37/558 [00:19<05:34,  1.56it/s]\u001b[A\n",
            "  7%|▋         | 38/558 [00:20<05:26,  1.59it/s]\u001b[A\n",
            "  7%|▋         | 39/558 [00:20<04:34,  1.89it/s]\u001b[A\n",
            "  7%|▋         | 40/558 [00:21<04:09,  2.08it/s]\u001b[A\n",
            "  7%|▋         | 41/558 [00:21<03:59,  2.16it/s]\u001b[A\n",
            "  8%|▊         | 42/558 [00:21<03:33,  2.42it/s]\u001b[A\n",
            "  8%|▊         | 43/558 [00:22<04:47,  1.79it/s]\u001b[A\n",
            "  8%|▊         | 44/558 [00:23<04:31,  1.89it/s]\u001b[A\n",
            "  8%|▊         | 45/558 [00:23<04:41,  1.82it/s]\u001b[A\n",
            "  8%|▊         | 46/558 [00:24<04:27,  1.91it/s]\u001b[A\n",
            "  8%|▊         | 47/558 [00:25<05:19,  1.60it/s]\u001b[A\n",
            "  9%|▊         | 48/558 [00:25<04:38,  1.83it/s]\u001b[A\n",
            "  9%|▉         | 49/558 [00:26<05:05,  1.66it/s]\u001b[A\n",
            "  9%|▉         | 50/558 [00:26<04:56,  1.71it/s]\u001b[A\n",
            "  9%|▉         | 51/558 [00:27<04:23,  1.92it/s]\u001b[A\n",
            "  9%|▉         | 52/558 [00:27<04:45,  1.77it/s]\u001b[A\n",
            "  9%|▉         | 53/558 [00:28<04:39,  1.81it/s]\u001b[A\n",
            " 10%|▉         | 54/558 [00:28<03:45,  2.23it/s]\u001b[A\n",
            " 10%|▉         | 55/558 [00:28<03:30,  2.39it/s]\u001b[A\n",
            " 10%|█         | 56/558 [00:29<03:32,  2.36it/s]\u001b[A\n",
            " 10%|█         | 57/558 [00:29<03:39,  2.29it/s]\u001b[A\n",
            " 10%|█         | 58/558 [00:30<03:38,  2.29it/s]\u001b[A\n",
            " 11%|█         | 59/558 [00:30<03:19,  2.50it/s]\u001b[A\n",
            " 11%|█         | 60/558 [00:31<03:40,  2.25it/s]\u001b[A\n",
            " 11%|█         | 61/558 [00:31<03:44,  2.22it/s]\u001b[A\n",
            " 11%|█         | 62/558 [00:31<03:30,  2.35it/s]\u001b[A\n",
            " 11%|█▏        | 63/558 [00:32<04:44,  1.74it/s]\u001b[A\n",
            " 11%|█▏        | 64/558 [00:33<04:45,  1.73it/s]\u001b[A\n",
            " 12%|█▏        | 65/558 [00:33<04:19,  1.90it/s]\u001b[A\n",
            " 12%|█▏        | 66/558 [00:34<03:48,  2.15it/s]\u001b[A\n",
            " 12%|█▏        | 67/558 [00:34<03:40,  2.22it/s]\u001b[A\n",
            " 12%|█▏        | 68/558 [00:35<04:11,  1.95it/s]\u001b[A\n",
            " 12%|█▏        | 69/558 [00:36<05:17,  1.54it/s]\u001b[A\n",
            " 13%|█▎        | 70/558 [00:37<06:22,  1.28it/s]\u001b[A\n",
            " 13%|█▎        | 71/558 [00:37<05:44,  1.41it/s]\u001b[A\n",
            " 13%|█▎        | 72/558 [00:38<05:11,  1.56it/s]\u001b[A\n",
            " 13%|█▎        | 73/558 [00:39<05:26,  1.49it/s]\u001b[A\n",
            " 13%|█▎        | 74/558 [00:39<04:53,  1.65it/s]\u001b[A\n",
            " 13%|█▎        | 75/558 [00:39<04:42,  1.71it/s]\u001b[A\n",
            " 14%|█▎        | 76/558 [00:40<04:44,  1.70it/s]\u001b[A\n",
            " 14%|█▍        | 77/558 [00:41<04:16,  1.87it/s]\u001b[A\n",
            " 14%|█▍        | 78/558 [00:42<07:03,  1.13it/s]\u001b[A\n",
            " 14%|█▍        | 79/558 [00:43<05:46,  1.38it/s]\u001b[A\n",
            " 14%|█▍        | 80/558 [00:43<05:17,  1.51it/s]\u001b[A\n",
            " 15%|█▍        | 81/558 [00:44<05:40,  1.40it/s]\u001b[A\n",
            " 15%|█▍        | 82/558 [00:44<04:58,  1.59it/s]\u001b[A\n",
            " 15%|█▍        | 83/558 [00:45<04:32,  1.74it/s]\u001b[A\n",
            " 15%|█▌        | 84/558 [00:45<03:46,  2.09it/s]\u001b[A\n",
            " 15%|█▌        | 85/558 [00:45<03:14,  2.43it/s]\u001b[A\n",
            " 15%|█▌        | 86/558 [00:46<02:56,  2.68it/s]\u001b[A\n",
            " 16%|█▌        | 87/558 [00:46<03:26,  2.28it/s]\u001b[A\n",
            " 16%|█▌        | 88/558 [00:47<03:57,  1.98it/s]\u001b[A\n",
            " 16%|█▌        | 89/558 [00:47<03:53,  2.00it/s]\u001b[A\n",
            " 16%|█▌        | 90/558 [00:48<03:56,  1.98it/s]\u001b[A\n",
            " 16%|█▋        | 91/558 [00:48<03:57,  1.96it/s]\u001b[A\n",
            " 16%|█▋        | 92/558 [00:49<04:58,  1.56it/s]\u001b[A\n",
            " 17%|█▋        | 93/558 [00:50<04:26,  1.74it/s]\u001b[A\n",
            " 17%|█▋        | 94/558 [00:50<04:06,  1.89it/s]\u001b[A\n",
            " 17%|█▋        | 95/558 [00:51<04:21,  1.77it/s]\u001b[A\n",
            " 17%|█▋        | 96/558 [00:51<04:07,  1.87it/s]\u001b[A\n",
            " 17%|█▋        | 97/558 [00:52<04:18,  1.79it/s]\u001b[A\n",
            " 18%|█▊        | 98/558 [00:53<04:31,  1.69it/s]\u001b[A\n",
            " 18%|█▊        | 99/558 [00:53<04:22,  1.75it/s]\u001b[A\n",
            " 18%|█▊        | 100/558 [00:54<04:10,  1.83it/s]\u001b[A\n",
            " 18%|█▊        | 101/558 [00:54<04:31,  1.69it/s]\u001b[A\n",
            " 18%|█▊        | 102/558 [00:55<04:15,  1.79it/s]\u001b[A\n",
            " 18%|█▊        | 103/558 [00:55<04:21,  1.74it/s]\u001b[A\n",
            " 19%|█▊        | 104/558 [00:56<03:44,  2.02it/s]\u001b[A\n",
            " 19%|█▉        | 105/558 [00:56<03:44,  2.02it/s]\u001b[A\n",
            " 19%|█▉        | 106/558 [00:57<03:32,  2.13it/s]\u001b[A\n",
            " 19%|█▉        | 107/558 [00:57<03:28,  2.16it/s]\u001b[A\n",
            " 19%|█▉        | 108/558 [00:58<04:16,  1.75it/s]\u001b[A\n",
            " 20%|█▉        | 109/558 [00:58<03:56,  1.90it/s]\u001b[A\n",
            " 20%|█▉        | 110/558 [00:59<03:37,  2.06it/s]\u001b[A\n",
            " 20%|█▉        | 111/558 [00:59<03:36,  2.06it/s]\u001b[A\n",
            " 20%|██        | 112/558 [01:00<03:25,  2.17it/s]\u001b[A\n",
            " 20%|██        | 113/558 [01:00<03:44,  1.98it/s]\u001b[A\n",
            " 20%|██        | 114/558 [01:01<04:00,  1.85it/s]\u001b[A\n",
            " 21%|██        | 115/558 [01:01<04:17,  1.72it/s]\u001b[A\n",
            " 21%|██        | 116/558 [01:02<03:55,  1.88it/s]\u001b[A\n",
            " 21%|██        | 117/558 [01:03<06:12,  1.18it/s]\u001b[A\n",
            " 21%|██        | 118/558 [01:04<05:30,  1.33it/s]\u001b[A\n",
            " 21%|██▏       | 119/558 [01:05<05:19,  1.37it/s]\u001b[A\n",
            " 22%|██▏       | 120/558 [01:05<04:42,  1.55it/s]\u001b[A\n",
            " 22%|██▏       | 121/558 [01:06<04:32,  1.61it/s]\u001b[A\n",
            " 22%|██▏       | 122/558 [01:06<03:58,  1.83it/s]\u001b[A\n",
            " 22%|██▏       | 123/558 [01:07<04:31,  1.60it/s]\u001b[A\n",
            " 22%|██▏       | 124/558 [01:07<03:57,  1.82it/s]\u001b[A\n",
            " 22%|██▏       | 125/558 [01:08<03:57,  1.82it/s]\u001b[A\n",
            " 23%|██▎       | 126/558 [01:08<03:54,  1.85it/s]\u001b[A\n",
            " 23%|██▎       | 127/558 [01:09<04:12,  1.71it/s]\u001b[A\n",
            " 23%|██▎       | 128/558 [01:09<03:56,  1.82it/s]\u001b[A\n",
            " 23%|██▎       | 129/558 [01:10<03:45,  1.90it/s]\u001b[A\n",
            " 23%|██▎       | 130/558 [01:10<03:41,  1.93it/s]\u001b[A\n",
            " 23%|██▎       | 131/558 [01:11<04:12,  1.69it/s]\u001b[A\n",
            " 24%|██▎       | 132/558 [01:12<04:18,  1.65it/s]\u001b[A\n",
            " 24%|██▍       | 133/558 [01:12<04:26,  1.59it/s]\u001b[A\n",
            " 24%|██▍       | 134/558 [01:13<03:58,  1.78it/s]\u001b[A\n",
            " 24%|██▍       | 135/558 [01:13<03:42,  1.90it/s]\u001b[A\n",
            " 24%|██▍       | 136/558 [01:14<03:30,  2.01it/s]\u001b[A\n",
            " 25%|██▍       | 137/558 [01:14<03:31,  1.99it/s]\u001b[A\n",
            " 25%|██▍       | 138/558 [01:15<03:09,  2.22it/s]\u001b[A\n",
            " 25%|██▍       | 139/558 [01:15<03:05,  2.25it/s]\u001b[A\n",
            " 25%|██▌       | 140/558 [01:16<03:14,  2.15it/s]\u001b[A\n",
            " 25%|██▌       | 141/558 [01:16<04:07,  1.69it/s]\u001b[A\n",
            " 25%|██▌       | 142/558 [01:17<03:43,  1.86it/s]\u001b[A\n",
            " 26%|██▌       | 143/558 [01:17<03:33,  1.94it/s]\u001b[A\n",
            " 26%|██▌       | 144/558 [01:18<03:53,  1.77it/s]\u001b[A\n",
            " 26%|██▌       | 145/558 [01:18<03:45,  1.83it/s]\u001b[A\n",
            " 26%|██▌       | 146/558 [01:19<03:43,  1.84it/s]\u001b[A\n",
            " 26%|██▋       | 147/558 [01:19<03:27,  1.98it/s]\u001b[A\n",
            " 27%|██▋       | 148/558 [01:20<03:28,  1.97it/s]\u001b[A\n",
            " 27%|██▋       | 149/558 [01:21<03:36,  1.89it/s]\u001b[A\n",
            " 27%|██▋       | 150/558 [01:21<04:09,  1.63it/s]\u001b[A\n",
            " 27%|██▋       | 151/558 [01:22<04:19,  1.57it/s]\u001b[A\n",
            " 27%|██▋       | 152/558 [01:23<04:02,  1.68it/s]\u001b[A\n",
            " 27%|██▋       | 153/558 [01:24<05:14,  1.29it/s]\u001b[A\n",
            " 28%|██▊       | 154/558 [01:24<05:01,  1.34it/s]\u001b[A\n",
            " 28%|██▊       | 155/558 [01:25<05:04,  1.32it/s]\u001b[A\n",
            " 28%|██▊       | 156/558 [01:25<04:08,  1.62it/s]\u001b[A\n",
            " 28%|██▊       | 157/558 [01:26<03:39,  1.82it/s]\u001b[A\n",
            " 28%|██▊       | 158/558 [01:26<03:34,  1.86it/s]\u001b[A\n",
            " 28%|██▊       | 159/558 [01:27<03:48,  1.75it/s]\u001b[A\n",
            " 29%|██▊       | 160/558 [01:28<03:37,  1.83it/s]\u001b[A\n",
            " 29%|██▉       | 161/558 [01:28<03:32,  1.87it/s]\u001b[A\n",
            " 29%|██▉       | 162/558 [01:28<03:18,  1.99it/s]\u001b[A\n",
            " 29%|██▉       | 163/558 [01:30<04:33,  1.45it/s]\u001b[A\n",
            " 29%|██▉       | 164/558 [01:30<04:19,  1.52it/s]\u001b[A\n",
            " 30%|██▉       | 165/558 [01:31<03:55,  1.67it/s]\u001b[A\n",
            " 30%|██▉       | 166/558 [01:31<04:22,  1.49it/s]\u001b[A\n",
            " 30%|██▉       | 167/558 [01:32<03:57,  1.65it/s]\u001b[A\n",
            " 30%|███       | 168/558 [01:33<04:03,  1.60it/s]\u001b[A\n",
            " 30%|███       | 169/558 [01:33<03:36,  1.79it/s]\u001b[A\n",
            " 30%|███       | 170/558 [01:34<03:36,  1.79it/s]\u001b[A\n",
            " 31%|███       | 171/558 [01:34<03:18,  1.94it/s]\u001b[A\n",
            " 31%|███       | 172/558 [01:34<03:12,  2.01it/s]\u001b[A\n",
            " 31%|███       | 173/558 [01:35<03:35,  1.79it/s]\u001b[A\n",
            " 31%|███       | 174/558 [01:36<03:26,  1.86it/s]\u001b[A\n",
            " 31%|███▏      | 175/558 [01:36<03:31,  1.81it/s]\u001b[A\n",
            " 32%|███▏      | 176/558 [01:37<03:32,  1.80it/s]\u001b[A\n",
            " 32%|███▏      | 177/558 [01:38<04:22,  1.45it/s]\u001b[A\n",
            " 32%|███▏      | 178/558 [01:38<04:15,  1.49it/s]\u001b[A\n",
            " 32%|███▏      | 179/558 [01:39<04:03,  1.55it/s]\u001b[A\n",
            " 32%|███▏      | 180/558 [01:39<03:21,  1.88it/s]\u001b[A\n",
            " 32%|███▏      | 181/558 [01:40<03:41,  1.70it/s]\u001b[A\n",
            " 33%|███▎      | 182/558 [01:40<03:26,  1.82it/s]\u001b[A\n",
            " 33%|███▎      | 183/558 [01:41<03:13,  1.94it/s]\u001b[A\n",
            " 33%|███▎      | 184/558 [01:41<02:52,  2.17it/s]\u001b[A\n",
            " 33%|███▎      | 185/558 [01:42<03:08,  1.98it/s]\u001b[A\n",
            " 33%|███▎      | 186/558 [01:42<03:27,  1.79it/s]\u001b[A\n",
            " 34%|███▎      | 187/558 [01:43<02:55,  2.12it/s]\u001b[A\n",
            " 34%|███▎      | 188/558 [01:43<02:45,  2.24it/s]\u001b[A\n",
            " 34%|███▍      | 189/558 [01:44<03:14,  1.90it/s]\u001b[A\n",
            " 34%|███▍      | 190/558 [01:45<03:30,  1.74it/s]\u001b[A\n",
            " 34%|███▍      | 191/558 [01:45<03:19,  1.84it/s]\u001b[A\n",
            " 34%|███▍      | 192/558 [01:46<03:17,  1.85it/s]\u001b[A\n",
            " 35%|███▍      | 193/558 [01:46<03:16,  1.86it/s]\u001b[A\n",
            " 35%|███▍      | 194/558 [01:46<02:57,  2.05it/s]\u001b[A\n",
            " 35%|███▍      | 195/558 [01:47<03:03,  1.98it/s]\u001b[A\n",
            " 35%|███▌      | 196/558 [01:47<02:57,  2.04it/s]\u001b[A\n",
            " 35%|███▌      | 197/558 [01:48<02:42,  2.22it/s]\u001b[A\n",
            " 35%|███▌      | 198/558 [01:48<02:49,  2.13it/s]\u001b[A\n",
            " 36%|███▌      | 199/558 [01:49<03:00,  1.98it/s]\u001b[A\n",
            " 36%|███▌      | 200/558 [01:49<02:52,  2.07it/s]\u001b[A\n",
            " 36%|███▌      | 201/558 [01:50<03:40,  1.62it/s]\u001b[A\n",
            " 36%|███▌      | 202/558 [01:52<05:09,  1.15it/s]\u001b[A\n",
            " 36%|███▋      | 203/558 [01:52<04:33,  1.30it/s]\u001b[A\n",
            " 37%|███▋      | 204/558 [01:54<06:24,  1.09s/it]\u001b[A\n",
            " 37%|███▋      | 205/558 [01:56<08:35,  1.46s/it]\u001b[A\n",
            " 37%|███▋      | 206/558 [01:57<07:25,  1.27s/it]\u001b[A\n",
            " 37%|███▋      | 207/558 [01:58<05:53,  1.01s/it]\u001b[A\n",
            " 37%|███▋      | 208/558 [01:59<06:17,  1.08s/it]\u001b[A\n",
            " 37%|███▋      | 209/558 [02:00<05:44,  1.01it/s]\u001b[A\n",
            " 38%|███▊      | 210/558 [02:00<04:51,  1.20it/s]\u001b[A\n",
            " 38%|███▊      | 211/558 [02:01<04:03,  1.43it/s]\u001b[A\n",
            " 38%|███▊      | 212/558 [02:01<03:26,  1.68it/s]\u001b[A\n",
            " 38%|███▊      | 213/558 [02:01<03:01,  1.90it/s]\u001b[A\n",
            " 38%|███▊      | 214/558 [02:02<02:42,  2.12it/s]\u001b[A\n",
            " 39%|███▊      | 215/558 [02:02<02:38,  2.17it/s]\u001b[A\n",
            " 39%|███▊      | 216/558 [02:03<02:42,  2.10it/s]\u001b[A\n",
            " 39%|███▉      | 217/558 [02:03<02:50,  2.00it/s]\u001b[A\n",
            " 39%|███▉      | 218/558 [02:04<03:38,  1.56it/s]\u001b[A\n",
            " 39%|███▉      | 219/558 [02:05<03:35,  1.57it/s]\u001b[A\n",
            " 39%|███▉      | 220/558 [02:05<03:27,  1.63it/s]\u001b[A\n",
            " 40%|███▉      | 221/558 [02:06<02:59,  1.88it/s]\u001b[A\n",
            " 40%|███▉      | 222/558 [02:06<02:44,  2.05it/s]\u001b[A\n",
            " 40%|███▉      | 223/558 [02:07<03:23,  1.65it/s]\u001b[A\n",
            " 40%|████      | 224/558 [02:07<03:24,  1.63it/s]\u001b[A\n",
            " 40%|████      | 225/558 [02:08<03:36,  1.54it/s]\u001b[A\n",
            " 41%|████      | 226/558 [02:09<03:31,  1.57it/s]\u001b[A\n",
            " 41%|████      | 227/558 [02:09<03:06,  1.78it/s]\u001b[A\n",
            " 41%|████      | 228/558 [02:10<02:45,  1.99it/s]\u001b[A\n",
            " 41%|████      | 229/558 [02:10<02:54,  1.89it/s]\u001b[A\n",
            " 41%|████      | 230/558 [02:11<02:51,  1.92it/s]\u001b[A\n",
            " 41%|████▏     | 231/558 [02:11<02:58,  1.83it/s]\u001b[A\n",
            " 42%|████▏     | 232/558 [02:12<03:04,  1.76it/s]\u001b[A\n",
            " 42%|████▏     | 233/558 [02:13<03:30,  1.54it/s]\u001b[A\n",
            " 42%|████▏     | 234/558 [02:13<03:00,  1.80it/s]\u001b[A\n",
            " 42%|████▏     | 235/558 [02:13<02:44,  1.96it/s]\u001b[A\n",
            " 42%|████▏     | 236/558 [02:14<02:35,  2.08it/s]\u001b[A\n",
            " 42%|████▏     | 237/558 [02:14<02:14,  2.39it/s]\u001b[A\n",
            " 43%|████▎     | 238/558 [02:15<02:34,  2.07it/s]\u001b[A\n",
            " 43%|████▎     | 239/558 [02:15<02:49,  1.89it/s]\u001b[A\n",
            " 43%|████▎     | 240/558 [02:16<02:43,  1.94it/s]\u001b[A\n",
            " 43%|████▎     | 241/558 [02:17<03:00,  1.76it/s]\u001b[A\n",
            " 43%|████▎     | 242/558 [02:17<02:52,  1.83it/s]\u001b[A\n",
            " 44%|████▎     | 243/558 [02:18<02:38,  1.99it/s]\u001b[A\n",
            " 44%|████▎     | 244/558 [02:18<02:51,  1.83it/s]\u001b[A\n",
            " 44%|████▍     | 245/558 [02:19<03:06,  1.68it/s]\u001b[A\n",
            " 44%|████▍     | 246/558 [02:20<03:22,  1.54it/s]\u001b[A\n",
            " 44%|████▍     | 247/558 [02:20<03:03,  1.70it/s]\u001b[A\n",
            " 44%|████▍     | 248/558 [02:21<02:55,  1.76it/s]\u001b[A\n",
            " 45%|████▍     | 249/558 [02:21<02:56,  1.75it/s]\u001b[A\n",
            " 45%|████▍     | 250/558 [02:23<05:11,  1.01s/it]\u001b[A\n",
            " 45%|████▍     | 251/558 [02:24<05:04,  1.01it/s]\u001b[A\n",
            " 45%|████▌     | 252/558 [02:25<04:25,  1.15it/s]\u001b[A\n",
            " 45%|████▌     | 253/558 [02:25<03:34,  1.42it/s]\u001b[A\n",
            " 46%|████▌     | 254/558 [02:26<03:27,  1.47it/s]\u001b[A\n",
            " 46%|████▌     | 255/558 [02:26<03:19,  1.52it/s]\u001b[A\n",
            " 46%|████▌     | 256/558 [02:27<03:24,  1.47it/s]\u001b[A\n",
            " 46%|████▌     | 257/558 [02:28<03:33,  1.41it/s]\u001b[A\n",
            " 46%|████▌     | 258/558 [02:29<03:37,  1.38it/s]\u001b[A\n",
            " 46%|████▋     | 259/558 [02:29<02:56,  1.69it/s]\u001b[A\n",
            " 47%|████▋     | 260/558 [02:29<02:53,  1.72it/s]\u001b[A\n",
            " 47%|████▋     | 261/558 [02:30<02:54,  1.71it/s]\u001b[A\n",
            " 47%|████▋     | 262/558 [02:31<02:47,  1.77it/s]\u001b[A\n",
            " 47%|████▋     | 263/558 [02:31<02:26,  2.01it/s]\u001b[A\n",
            " 47%|████▋     | 264/558 [02:32<02:39,  1.84it/s]\u001b[A\n",
            " 47%|████▋     | 265/558 [02:32<02:27,  1.99it/s]\u001b[A\n",
            " 48%|████▊     | 266/558 [02:32<02:29,  1.95it/s]\u001b[A\n",
            " 48%|████▊     | 267/558 [02:33<02:26,  1.99it/s]\u001b[A\n",
            " 48%|████▊     | 268/558 [02:34<02:35,  1.87it/s]\u001b[A\n",
            " 48%|████▊     | 269/558 [02:34<02:31,  1.90it/s]\u001b[A\n",
            " 48%|████▊     | 270/558 [02:35<02:31,  1.90it/s]\u001b[A\n",
            " 49%|████▊     | 271/558 [02:35<02:30,  1.91it/s]\u001b[A\n",
            " 49%|████▊     | 272/558 [02:36<02:20,  2.04it/s]\u001b[A\n",
            " 49%|████▉     | 273/558 [02:36<01:58,  2.40it/s]\u001b[A\n",
            " 49%|████▉     | 274/558 [02:37<02:43,  1.74it/s]\u001b[A\n",
            " 49%|████▉     | 275/558 [02:37<02:20,  2.02it/s]\u001b[A\n",
            " 49%|████▉     | 276/558 [02:38<02:21,  1.99it/s]\u001b[A\n",
            " 50%|████▉     | 277/558 [02:39<03:22,  1.39it/s]\u001b[A\n",
            " 50%|████▉     | 278/558 [02:39<03:03,  1.53it/s]\u001b[A\n",
            " 50%|█████     | 279/558 [02:40<02:58,  1.56it/s]\u001b[A\n",
            " 50%|█████     | 280/558 [02:41<03:07,  1.48it/s]\u001b[A\n",
            " 50%|█████     | 281/558 [02:41<03:00,  1.53it/s]\u001b[A\n",
            " 51%|█████     | 282/558 [02:42<03:06,  1.48it/s]\u001b[A\n",
            " 51%|█████     | 283/558 [02:43<03:02,  1.50it/s]\u001b[A\n",
            " 51%|█████     | 284/558 [02:44<03:24,  1.34it/s]\u001b[A\n",
            " 51%|█████     | 285/558 [02:44<03:16,  1.39it/s]\u001b[A\n",
            " 51%|█████▏    | 286/558 [02:45<03:28,  1.30it/s]\u001b[A\n",
            " 51%|█████▏    | 287/558 [02:46<03:02,  1.48it/s]\u001b[A\n",
            " 52%|█████▏    | 288/558 [02:46<02:26,  1.85it/s]\u001b[A\n",
            " 52%|█████▏    | 289/558 [02:46<02:31,  1.77it/s]\u001b[A\n",
            " 52%|█████▏    | 290/558 [02:47<02:13,  2.01it/s]\u001b[A\n",
            " 52%|█████▏    | 291/558 [02:47<02:23,  1.86it/s]\u001b[A\n",
            " 52%|█████▏    | 292/558 [02:48<02:06,  2.11it/s]\u001b[A\n",
            " 53%|█████▎    | 293/558 [02:48<02:05,  2.11it/s]\u001b[A\n",
            " 53%|█████▎    | 294/558 [02:49<02:04,  2.13it/s]\u001b[A\n",
            " 53%|█████▎    | 295/558 [02:49<02:04,  2.11it/s]\u001b[A\n",
            " 53%|█████▎    | 296/558 [02:50<02:11,  1.99it/s]\u001b[A\n",
            " 53%|█████▎    | 297/558 [02:50<02:13,  1.96it/s]\u001b[A\n",
            " 53%|█████▎    | 298/558 [02:51<02:14,  1.94it/s]\u001b[A\n",
            " 54%|█████▎    | 299/558 [02:51<02:15,  1.91it/s]\u001b[A\n",
            " 54%|█████▍    | 300/558 [02:52<02:24,  1.78it/s]\u001b[A\n",
            " 54%|█████▍    | 301/558 [02:53<02:27,  1.74it/s]\u001b[A\n",
            " 54%|█████▍    | 302/558 [02:53<02:21,  1.81it/s]\u001b[A\n",
            " 54%|█████▍    | 303/558 [02:53<02:09,  1.97it/s]\u001b[A\n",
            " 54%|█████▍    | 304/558 [02:54<02:09,  1.97it/s]\u001b[A\n",
            " 55%|█████▍    | 305/558 [02:55<02:23,  1.76it/s]\u001b[A\n",
            " 55%|█████▍    | 306/558 [02:55<02:29,  1.69it/s]\u001b[A\n",
            " 55%|█████▌    | 307/558 [02:56<02:13,  1.88it/s]\u001b[A\n",
            " 55%|█████▌    | 308/558 [02:56<02:26,  1.70it/s]\u001b[A\n",
            " 55%|█████▌    | 309/558 [02:57<02:21,  1.76it/s]\u001b[A\n",
            " 56%|█████▌    | 310/558 [02:57<02:18,  1.79it/s]\u001b[A\n",
            " 56%|█████▌    | 311/558 [02:58<02:13,  1.86it/s]\u001b[A\n",
            " 56%|█████▌    | 312/558 [02:58<02:04,  1.97it/s]\u001b[A\n",
            " 56%|█████▌    | 313/558 [02:59<01:58,  2.06it/s]\u001b[A\n",
            " 56%|█████▋    | 314/558 [02:59<02:06,  1.93it/s]\u001b[A\n",
            " 56%|█████▋    | 315/558 [03:00<02:12,  1.84it/s]\u001b[A\n",
            " 57%|█████▋    | 316/558 [03:00<01:59,  2.03it/s]\u001b[A\n",
            " 57%|█████▋    | 317/558 [03:01<01:52,  2.14it/s]\u001b[A\n",
            " 57%|█████▋    | 318/558 [03:01<02:00,  1.99it/s]\u001b[A\n",
            " 57%|█████▋    | 319/558 [03:02<01:56,  2.05it/s]\u001b[A\n",
            " 57%|█████▋    | 320/558 [03:02<01:53,  2.10it/s]\u001b[A\n",
            " 58%|█████▊    | 321/558 [03:03<01:51,  2.13it/s]\u001b[A\n",
            " 58%|█████▊    | 322/558 [03:03<02:08,  1.84it/s]\u001b[A\n",
            " 58%|█████▊    | 323/558 [03:04<02:11,  1.79it/s]\u001b[A\n",
            " 58%|█████▊    | 324/558 [03:05<02:07,  1.83it/s]\u001b[A\n",
            " 58%|█████▊    | 325/558 [03:05<02:00,  1.94it/s]\u001b[A\n",
            " 58%|█████▊    | 326/558 [03:06<02:21,  1.64it/s]\u001b[A\n",
            " 59%|█████▊    | 327/558 [03:06<02:15,  1.71it/s]\u001b[A\n",
            " 59%|█████▉    | 328/558 [03:07<02:12,  1.74it/s]\u001b[A\n",
            " 59%|█████▉    | 329/558 [03:08<02:18,  1.66it/s]\u001b[A\n",
            " 59%|█████▉    | 330/558 [03:08<01:58,  1.93it/s]\u001b[A\n",
            " 59%|█████▉    | 331/558 [03:08<02:00,  1.88it/s]\u001b[A\n",
            " 59%|█████▉    | 332/558 [03:09<02:07,  1.77it/s]\u001b[A\n",
            " 60%|█████▉    | 333/558 [03:10<02:09,  1.74it/s]\u001b[A\n",
            " 60%|█████▉    | 334/558 [03:10<02:03,  1.81it/s]\u001b[A\n",
            " 60%|██████    | 335/558 [03:10<01:44,  2.13it/s]\u001b[A\n",
            " 60%|██████    | 336/558 [03:11<02:02,  1.82it/s]\u001b[A\n",
            " 60%|██████    | 337/558 [03:12<02:17,  1.61it/s]\u001b[A\n",
            " 61%|██████    | 338/558 [03:12<02:01,  1.82it/s]\u001b[A\n",
            " 61%|██████    | 339/558 [03:13<01:52,  1.94it/s]\u001b[A\n",
            " 61%|██████    | 340/558 [03:13<01:46,  2.05it/s]\u001b[A\n",
            " 61%|██████    | 341/558 [03:14<01:58,  1.83it/s]\u001b[A\n",
            " 61%|██████▏   | 342/558 [03:14<01:47,  2.02it/s]\u001b[A\n",
            " 61%|██████▏   | 343/558 [03:15<01:40,  2.13it/s]\u001b[A\n",
            " 62%|██████▏   | 344/558 [03:15<01:41,  2.11it/s]\u001b[A\n",
            " 62%|██████▏   | 345/558 [03:16<01:52,  1.90it/s]\u001b[A\n",
            " 62%|██████▏   | 346/558 [03:17<02:26,  1.45it/s]\u001b[A\n",
            " 62%|██████▏   | 347/558 [03:17<02:09,  1.63it/s]\u001b[A\n",
            " 62%|██████▏   | 348/558 [03:18<02:02,  1.72it/s]\u001b[A\n",
            " 63%|██████▎   | 349/558 [03:18<01:49,  1.90it/s]\u001b[A\n",
            " 63%|██████▎   | 350/558 [03:19<01:32,  2.24it/s]\u001b[A\n",
            " 63%|██████▎   | 351/558 [03:19<01:51,  1.86it/s]\u001b[A\n",
            " 63%|██████▎   | 352/558 [03:20<02:06,  1.62it/s]\u001b[A\n",
            " 63%|██████▎   | 353/558 [03:21<02:08,  1.59it/s]\u001b[A\n",
            " 63%|██████▎   | 354/558 [03:21<02:15,  1.51it/s]\u001b[A\n",
            " 64%|██████▎   | 355/558 [03:22<02:01,  1.67it/s]\u001b[A\n",
            " 64%|██████▍   | 356/558 [03:23<02:10,  1.55it/s]\u001b[A\n",
            " 64%|██████▍   | 357/558 [03:23<01:43,  1.95it/s]\u001b[A\n",
            " 64%|██████▍   | 358/558 [03:23<01:44,  1.91it/s]\u001b[A\n",
            " 64%|██████▍   | 359/558 [03:24<01:32,  2.14it/s]\u001b[A\n",
            " 65%|██████▍   | 360/558 [03:24<01:27,  2.25it/s]\u001b[A\n",
            " 65%|██████▍   | 361/558 [03:25<01:32,  2.14it/s]\u001b[A\n",
            " 65%|██████▍   | 362/558 [03:25<01:25,  2.30it/s]\u001b[A\n",
            " 65%|██████▌   | 363/558 [03:26<01:33,  2.08it/s]\u001b[A\n",
            " 65%|██████▌   | 364/558 [03:26<01:41,  1.92it/s]\u001b[A\n",
            " 65%|██████▌   | 365/558 [03:27<01:41,  1.91it/s]\u001b[A\n",
            " 66%|██████▌   | 366/558 [03:27<01:41,  1.89it/s]\u001b[A\n",
            " 66%|██████▌   | 367/558 [03:28<01:30,  2.10it/s]\u001b[A\n",
            " 66%|██████▌   | 368/558 [03:28<01:28,  2.15it/s]\u001b[A\n",
            " 66%|██████▌   | 369/558 [03:29<01:29,  2.12it/s]\u001b[A\n",
            " 66%|██████▋   | 370/558 [03:29<01:34,  2.00it/s]\u001b[A\n",
            " 66%|██████▋   | 371/558 [03:30<01:44,  1.79it/s]\u001b[A\n",
            " 67%|██████▋   | 372/558 [03:30<01:33,  1.99it/s]\u001b[A\n",
            " 67%|██████▋   | 373/558 [03:31<01:27,  2.11it/s]\u001b[A\n",
            " 67%|██████▋   | 374/558 [03:31<01:27,  2.10it/s]\u001b[A\n",
            " 67%|██████▋   | 375/558 [03:31<01:18,  2.33it/s]\u001b[A\n",
            " 67%|██████▋   | 376/558 [03:32<01:29,  2.02it/s]\u001b[A\n",
            " 68%|██████▊   | 377/558 [03:33<01:25,  2.11it/s]\u001b[A\n",
            " 68%|██████▊   | 378/558 [03:33<01:18,  2.30it/s]\u001b[A\n",
            " 68%|██████▊   | 379/558 [03:33<01:15,  2.36it/s]\u001b[A\n",
            " 68%|██████▊   | 380/558 [03:34<01:35,  1.86it/s]\u001b[A\n",
            " 68%|██████▊   | 381/558 [03:34<01:30,  1.97it/s]\u001b[A\n",
            " 68%|██████▊   | 382/558 [03:35<01:29,  1.97it/s]\u001b[A\n",
            " 69%|██████▊   | 383/558 [03:36<01:54,  1.52it/s]\u001b[A\n",
            " 69%|██████▉   | 384/558 [03:37<01:55,  1.51it/s]\u001b[A\n",
            " 69%|██████▉   | 385/558 [03:37<01:45,  1.64it/s]\u001b[A\n",
            " 69%|██████▉   | 386/558 [03:38<01:49,  1.57it/s]\u001b[A\n",
            " 69%|██████▉   | 387/558 [03:38<01:39,  1.71it/s]\u001b[A\n",
            " 70%|██████▉   | 388/558 [03:39<01:32,  1.84it/s]\u001b[A\n",
            " 70%|██████▉   | 389/558 [03:39<01:28,  1.92it/s]\u001b[A\n",
            " 70%|██████▉   | 390/558 [03:40<01:25,  1.96it/s]\u001b[A\n",
            " 70%|███████   | 391/558 [03:41<01:56,  1.43it/s]\u001b[A\n",
            " 70%|███████   | 392/558 [03:41<01:42,  1.63it/s]\u001b[A\n",
            " 70%|███████   | 393/558 [03:42<01:49,  1.50it/s]\u001b[A\n",
            " 71%|███████   | 394/558 [03:43<01:55,  1.42it/s]\u001b[A\n",
            " 71%|███████   | 395/558 [03:44<02:01,  1.34it/s]\u001b[A\n",
            " 71%|███████   | 396/558 [03:44<01:59,  1.35it/s]\u001b[A\n",
            " 71%|███████   | 397/558 [03:45<01:54,  1.40it/s]\u001b[A\n",
            " 71%|███████▏  | 398/558 [03:46<01:43,  1.54it/s]\u001b[A\n",
            " 72%|███████▏  | 399/558 [03:46<01:43,  1.54it/s]\u001b[A\n",
            " 72%|███████▏  | 400/558 [03:47<01:59,  1.32it/s]\u001b[A\n",
            " 72%|███████▏  | 401/558 [03:48<01:47,  1.46it/s]\u001b[A\n",
            " 72%|███████▏  | 402/558 [03:48<01:28,  1.76it/s]\u001b[A\n",
            " 72%|███████▏  | 403/558 [03:49<01:32,  1.67it/s]\u001b[A\n",
            " 72%|███████▏  | 404/558 [03:49<01:23,  1.83it/s]\u001b[A\n",
            " 73%|███████▎  | 405/558 [03:50<01:17,  1.99it/s]\u001b[A\n",
            " 73%|███████▎  | 406/558 [03:50<01:11,  2.13it/s]\u001b[A\n",
            " 73%|███████▎  | 407/558 [03:50<01:10,  2.16it/s]\u001b[A\n",
            " 73%|███████▎  | 408/558 [03:51<01:06,  2.27it/s]\u001b[A\n",
            " 73%|███████▎  | 409/558 [03:52<01:31,  1.63it/s]\u001b[A\n",
            " 73%|███████▎  | 410/558 [03:53<01:41,  1.45it/s]\u001b[A\n",
            " 74%|███████▎  | 411/558 [03:53<01:27,  1.68it/s]\u001b[A\n",
            " 74%|███████▍  | 412/558 [03:54<01:28,  1.66it/s]\u001b[A\n",
            " 74%|███████▍  | 413/558 [03:54<01:31,  1.58it/s]\u001b[A\n",
            " 74%|███████▍  | 414/558 [03:55<01:41,  1.42it/s]\u001b[A\n",
            " 74%|███████▍  | 415/558 [03:56<01:36,  1.47it/s]\u001b[A\n",
            " 75%|███████▍  | 416/558 [03:56<01:26,  1.64it/s]\u001b[A\n",
            " 75%|███████▍  | 417/558 [03:57<01:21,  1.73it/s]\u001b[A\n",
            " 75%|███████▍  | 418/558 [03:58<01:53,  1.23it/s]\u001b[A\n",
            " 75%|███████▌  | 419/558 [03:59<01:58,  1.17it/s]\u001b[A\n",
            " 75%|███████▌  | 420/558 [03:59<01:35,  1.44it/s]\u001b[A\n",
            " 75%|███████▌  | 421/558 [04:00<01:28,  1.55it/s]\u001b[A\n",
            " 76%|███████▌  | 422/558 [04:01<01:24,  1.61it/s]\u001b[A\n",
            " 76%|███████▌  | 423/558 [04:01<01:15,  1.79it/s]\u001b[A\n",
            " 76%|███████▌  | 424/558 [04:01<01:12,  1.85it/s]\u001b[A\n",
            " 76%|███████▌  | 425/558 [04:02<01:13,  1.81it/s]\u001b[A\n",
            " 76%|███████▋  | 426/558 [04:03<01:11,  1.83it/s]\u001b[A\n",
            " 77%|███████▋  | 427/558 [04:03<01:03,  2.08it/s]\u001b[A\n",
            " 77%|███████▋  | 428/558 [04:04<01:14,  1.75it/s]\u001b[A\n",
            " 77%|███████▋  | 429/558 [04:05<01:56,  1.11it/s]\u001b[A\n",
            " 77%|███████▋  | 430/558 [04:07<02:08,  1.00s/it]\u001b[A\n",
            " 77%|███████▋  | 431/558 [04:07<01:42,  1.24it/s]\u001b[A\n",
            " 77%|███████▋  | 432/558 [04:08<01:37,  1.30it/s]\u001b[A\n",
            " 78%|███████▊  | 433/558 [04:08<01:27,  1.43it/s]\u001b[A\n",
            " 78%|███████▊  | 434/558 [04:09<01:23,  1.49it/s]\u001b[A\n",
            " 78%|███████▊  | 435/558 [04:09<01:10,  1.76it/s]\u001b[A\n",
            " 78%|███████▊  | 436/558 [04:10<01:12,  1.68it/s]\u001b[A\n",
            " 78%|███████▊  | 437/558 [04:10<01:13,  1.66it/s]\u001b[A\n",
            " 78%|███████▊  | 438/558 [04:11<01:22,  1.46it/s]\u001b[A\n",
            " 79%|███████▊  | 439/558 [04:12<01:12,  1.63it/s]\u001b[A\n",
            " 79%|███████▉  | 440/558 [04:12<01:09,  1.71it/s]\u001b[A\n",
            " 79%|███████▉  | 441/558 [04:13<01:08,  1.72it/s]\u001b[A\n",
            " 79%|███████▉  | 442/558 [04:13<01:08,  1.69it/s]\u001b[A\n",
            " 79%|███████▉  | 443/558 [04:14<01:10,  1.64it/s]\u001b[A\n",
            " 80%|███████▉  | 444/558 [04:15<01:11,  1.59it/s]\u001b[A\n",
            " 80%|███████▉  | 445/558 [04:16<01:28,  1.28it/s]\u001b[A\n",
            " 80%|███████▉  | 446/558 [04:16<01:17,  1.45it/s]\u001b[A\n",
            " 80%|████████  | 447/558 [04:17<01:08,  1.61it/s]\u001b[A\n",
            " 80%|████████  | 448/558 [04:17<01:04,  1.70it/s]\u001b[A\n",
            " 80%|████████  | 449/558 [04:18<01:17,  1.41it/s]\u001b[A\n",
            " 81%|████████  | 450/558 [04:19<01:26,  1.24it/s]\u001b[A\n",
            " 81%|████████  | 451/558 [04:20<01:14,  1.43it/s]\u001b[A\n",
            " 81%|████████  | 452/558 [04:20<01:06,  1.59it/s]\u001b[A\n",
            " 81%|████████  | 453/558 [04:21<01:03,  1.65it/s]\u001b[A\n",
            " 81%|████████▏ | 454/558 [04:21<00:59,  1.76it/s]\u001b[A\n",
            " 82%|████████▏ | 455/558 [04:22<01:01,  1.68it/s]\u001b[A\n",
            " 82%|████████▏ | 456/558 [04:23<01:01,  1.65it/s]\u001b[A\n",
            " 82%|████████▏ | 457/558 [04:23<00:58,  1.73it/s]\u001b[A\n",
            " 82%|████████▏ | 458/558 [04:24<00:56,  1.77it/s]\u001b[A\n",
            " 82%|████████▏ | 459/558 [04:24<00:49,  2.01it/s]\u001b[A\n",
            " 82%|████████▏ | 460/558 [04:25<00:58,  1.66it/s]\u001b[A\n",
            " 83%|████████▎ | 461/558 [04:26<01:01,  1.57it/s]\u001b[A\n",
            " 83%|████████▎ | 462/558 [04:26<01:11,  1.35it/s]\u001b[A\n",
            " 83%|████████▎ | 463/558 [04:27<01:05,  1.45it/s]\u001b[A\n",
            " 83%|████████▎ | 464/558 [04:28<01:13,  1.28it/s]\u001b[A\n",
            " 83%|████████▎ | 465/558 [04:29<01:14,  1.25it/s]\u001b[A\n",
            " 84%|████████▎ | 466/558 [04:29<01:06,  1.38it/s]\u001b[A\n",
            " 84%|████████▎ | 467/558 [04:31<01:34,  1.03s/it]\u001b[A\n",
            " 84%|████████▍ | 468/558 [04:32<01:23,  1.08it/s]\u001b[A\n",
            " 84%|████████▍ | 469/558 [04:32<01:13,  1.22it/s]\u001b[A\n",
            " 84%|████████▍ | 470/558 [04:33<01:03,  1.38it/s]\u001b[A\n",
            " 84%|████████▍ | 471/558 [04:34<01:04,  1.35it/s]\u001b[A\n",
            " 85%|████████▍ | 472/558 [04:34<00:53,  1.59it/s]\u001b[A\n",
            " 85%|████████▍ | 473/558 [04:35<00:54,  1.56it/s]\u001b[A\n",
            " 85%|████████▍ | 474/558 [04:35<00:50,  1.68it/s]\u001b[A\n",
            " 85%|████████▌ | 475/558 [04:36<00:46,  1.79it/s]\u001b[A\n",
            " 85%|████████▌ | 476/558 [04:36<00:46,  1.76it/s]\u001b[A\n",
            " 85%|████████▌ | 477/558 [04:37<00:48,  1.65it/s]\u001b[A\n",
            " 86%|████████▌ | 478/558 [04:37<00:44,  1.81it/s]\u001b[A\n",
            " 86%|████████▌ | 479/558 [04:38<00:40,  1.96it/s]\u001b[A\n",
            " 86%|████████▌ | 480/558 [04:38<00:34,  2.26it/s]\u001b[A\n",
            " 86%|████████▌ | 481/558 [04:39<00:35,  2.17it/s]\u001b[A\n",
            " 86%|████████▋ | 482/558 [04:39<00:32,  2.32it/s]\u001b[A\n",
            " 87%|████████▋ | 483/558 [04:39<00:32,  2.32it/s]\u001b[A\n",
            " 87%|████████▋ | 484/558 [04:40<00:32,  2.30it/s]\u001b[A\n",
            " 87%|████████▋ | 485/558 [04:40<00:30,  2.42it/s]\u001b[A\n",
            " 87%|████████▋ | 486/558 [04:41<00:29,  2.43it/s]\u001b[A\n",
            " 87%|████████▋ | 487/558 [04:41<00:29,  2.39it/s]\u001b[A\n",
            " 87%|████████▋ | 488/558 [04:42<00:29,  2.34it/s]\u001b[A\n",
            " 88%|████████▊ | 489/558 [04:42<00:32,  2.15it/s]\u001b[A\n",
            " 88%|████████▊ | 490/558 [04:43<00:31,  2.19it/s]\u001b[A\n",
            " 88%|████████▊ | 491/558 [04:43<00:28,  2.32it/s]\u001b[A\n",
            " 88%|████████▊ | 492/558 [04:43<00:29,  2.27it/s]\u001b[A\n",
            " 88%|████████▊ | 493/558 [04:44<00:29,  2.18it/s]\u001b[A\n",
            " 89%|████████▊ | 494/558 [04:44<00:29,  2.15it/s]\u001b[A\n",
            " 89%|████████▊ | 495/558 [04:45<00:28,  2.21it/s]\u001b[A\n",
            " 89%|████████▉ | 496/558 [04:45<00:28,  2.19it/s]\u001b[A\n",
            " 89%|████████▉ | 497/558 [04:46<00:26,  2.29it/s]\u001b[A\n",
            " 89%|████████▉ | 498/558 [04:46<00:26,  2.25it/s]\u001b[A\n",
            " 89%|████████▉ | 499/558 [04:47<00:26,  2.19it/s]\u001b[A\n",
            " 90%|████████▉ | 500/558 [04:47<00:27,  2.13it/s]\u001b[A\n",
            " 90%|████████▉ | 501/558 [04:48<00:26,  2.13it/s]\u001b[A\n",
            " 90%|████████▉ | 502/558 [04:48<00:25,  2.20it/s]\u001b[A\n",
            " 90%|█████████ | 503/558 [04:48<00:23,  2.29it/s]\u001b[A\n",
            " 90%|█████████ | 504/558 [04:49<00:23,  2.26it/s]\u001b[A\n",
            " 91%|█████████ | 505/558 [04:49<00:23,  2.29it/s]\u001b[A\n",
            " 91%|█████████ | 506/558 [04:50<00:22,  2.28it/s]\u001b[A\n",
            " 91%|█████████ | 507/558 [04:50<00:23,  2.18it/s]\u001b[A\n",
            " 91%|█████████ | 508/558 [04:51<00:22,  2.19it/s]\u001b[A\n",
            " 91%|█████████ | 509/558 [04:51<00:22,  2.17it/s]\u001b[A\n",
            " 91%|█████████▏| 510/558 [04:52<00:21,  2.19it/s]\u001b[A\n",
            " 92%|█████████▏| 511/558 [04:52<00:23,  2.03it/s]\u001b[A\n",
            " 92%|█████████▏| 512/558 [04:53<00:21,  2.10it/s]\u001b[A\n",
            " 92%|█████████▏| 513/558 [04:53<00:21,  2.10it/s]\u001b[A\n",
            " 92%|█████████▏| 514/558 [04:54<00:20,  2.12it/s]\u001b[A\n",
            " 92%|█████████▏| 515/558 [04:54<00:19,  2.17it/s]\u001b[A\n",
            " 92%|█████████▏| 516/558 [04:54<00:19,  2.19it/s]\u001b[A\n",
            " 93%|█████████▎| 517/558 [04:55<00:20,  2.01it/s]\u001b[A\n",
            " 93%|█████████▎| 518/558 [04:56<00:20,  1.96it/s]\u001b[A\n",
            " 93%|█████████▎| 519/558 [04:56<00:18,  2.14it/s]\u001b[A\n",
            " 93%|█████████▎| 520/558 [04:56<00:17,  2.22it/s]\u001b[A\n",
            " 93%|█████████▎| 521/558 [04:57<00:17,  2.17it/s]\u001b[A\n",
            " 94%|█████████▎| 522/558 [04:57<00:17,  2.08it/s]\u001b[A\n",
            " 94%|█████████▎| 523/558 [04:58<00:17,  2.05it/s]\u001b[A\n",
            " 94%|█████████▍| 524/558 [04:58<00:15,  2.18it/s]\u001b[A\n",
            " 94%|█████████▍| 525/558 [04:59<00:15,  2.10it/s]\u001b[A\n",
            " 94%|█████████▍| 526/558 [04:59<00:14,  2.16it/s]\u001b[A\n",
            " 94%|█████████▍| 527/558 [05:00<00:16,  1.92it/s]\u001b[A\n",
            " 95%|█████████▍| 528/558 [05:00<00:14,  2.05it/s]\u001b[A\n",
            " 95%|█████████▍| 529/558 [05:01<00:12,  2.30it/s]\u001b[A\n",
            " 95%|█████████▍| 530/558 [05:01<00:12,  2.22it/s]\u001b[A\n",
            " 95%|█████████▌| 531/558 [05:01<00:11,  2.31it/s]\u001b[A\n",
            " 95%|█████████▌| 532/558 [05:02<00:10,  2.55it/s]\u001b[A\n",
            " 96%|█████████▌| 533/558 [05:02<00:10,  2.49it/s]\u001b[A\n",
            " 96%|█████████▌| 534/558 [05:03<00:09,  2.53it/s]\u001b[A\n",
            " 96%|█████████▌| 535/558 [05:03<00:08,  2.57it/s]\u001b[A\n",
            " 96%|█████████▌| 536/558 [05:04<00:10,  2.08it/s]\u001b[A\n",
            " 96%|█████████▌| 537/558 [05:04<00:10,  2.09it/s]\u001b[A\n",
            " 96%|█████████▋| 538/558 [05:05<00:09,  2.11it/s]\u001b[A\n",
            " 97%|█████████▋| 539/558 [05:05<00:08,  2.13it/s]\u001b[A\n",
            " 97%|█████████▋| 540/558 [05:05<00:08,  2.12it/s]\u001b[A\n",
            " 97%|█████████▋| 541/558 [05:06<00:08,  2.12it/s]\u001b[A\n",
            " 97%|█████████▋| 542/558 [05:06<00:07,  2.02it/s]\u001b[A\n",
            " 97%|█████████▋| 543/558 [05:07<00:06,  2.15it/s]\u001b[A\n",
            " 97%|█████████▋| 544/558 [05:07<00:06,  2.26it/s]\u001b[A\n",
            " 98%|█████████▊| 545/558 [05:08<00:05,  2.47it/s]\u001b[A\n",
            " 98%|█████████▊| 546/558 [05:08<00:05,  2.37it/s]\u001b[A\n",
            " 98%|█████████▊| 547/558 [05:09<00:04,  2.20it/s]\u001b[A\n",
            " 98%|█████████▊| 548/558 [05:09<00:04,  2.16it/s]\u001b[A\n",
            " 98%|█████████▊| 549/558 [05:10<00:04,  2.05it/s]\u001b[A\n",
            " 99%|█████████▊| 550/558 [05:10<00:04,  1.99it/s]\u001b[A\n",
            " 99%|█████████▊| 551/558 [05:11<00:03,  2.10it/s]\u001b[A\n",
            " 99%|█████████▉| 552/558 [05:11<00:02,  2.02it/s]\u001b[A\n",
            " 99%|█████████▉| 553/558 [05:12<00:02,  1.80it/s]\u001b[A\n",
            " 99%|█████████▉| 554/558 [05:13<00:02,  1.66it/s]\u001b[A\n",
            " 99%|█████████▉| 555/558 [05:13<00:01,  1.81it/s]\u001b[A\n",
            "100%|█████████▉| 556/558 [05:14<00:01,  1.72it/s]\u001b[A\n",
            "100%|█████████▉| 557/558 [05:14<00:00,  1.79it/s]\u001b[A\n",
            "100%|██████████| 558/558 [05:14<00:00,  1.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x4bjKwYZu1O"
      },
      "source": [
        "Заменяем найденные сущности на такие же сущности, но через _ в тексте отзывов. Извлекаем биграммы, ранжируем с помощью трех коллокационных метрик (pmi, log likelyhood, dice). Сначала посмотрим, что получается для сущностей без дескрипторов, потом -- что для сущностей с дескрипторами."
      ],
      "id": "9x4bjKwYZu1O"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WscRXeCuMXdG",
        "outputId": "610591e1-94f8-4879-d9a2-484a1cedccd3"
      },
      "source": [
        "chunks_new = []\n",
        "for chunk in tqdm(chunks_prep1):\n",
        "  for ne in nes_prep:\n",
        "    ne_split = ne.split()\n",
        "    if ne in chunk:\n",
        "      chunk = chunk.replace(ne, '_'.join(ne_split))\n",
        "  chunks_new.append(chunk)"
      ],
      "id": "WscRXeCuMXdG",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/558 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 558/558 [00:00<00:00, 3147.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMUapvUAZrTQ",
        "outputId": "99d5a15e-d9f1-4366-c1da-ed3901a2d86c"
      },
      "source": [
        "list_metrics = {}\n",
        "\n",
        "for ner in tqdm(nes_split):\n",
        "  bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "  creature_filter = lambda *w: ner not in w \n",
        "  finder = BigramCollocationFinder.from_words(' '.join(chunks_new).split())\n",
        "  finder.apply_freq_filter(2)\n",
        "  finder.apply_ngram_filter(creature_filter)\n",
        "  list_metrics[ner] = (finder.nbest(bigram_measures.pmi, 15), finder.nbest(bigram_measures.likelihood_ratio, 15), finder.nbest(bigram_measures.dice, 15))"
      ],
      "id": "wMUapvUAZrTQ",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  5%|▌         | 1/20 [00:04<01:29,  4.72s/it]\u001b[A\n",
            " 10%|█         | 2/20 [00:09<01:24,  4.71s/it]\u001b[A\n",
            " 15%|█▌        | 3/20 [00:14<01:19,  4.70s/it]\u001b[A\n",
            " 20%|██        | 4/20 [00:18<01:15,  4.71s/it]\u001b[A\n",
            " 25%|██▌       | 5/20 [00:23<01:10,  4.70s/it]\u001b[A\n",
            " 30%|███       | 6/20 [00:28<01:05,  4.71s/it]\u001b[A\n",
            " 35%|███▌      | 7/20 [00:33<01:04,  4.98s/it]\u001b[A\n",
            " 40%|████      | 8/20 [00:39<01:02,  5.18s/it]\u001b[A\n",
            " 45%|████▌     | 9/20 [00:44<00:56,  5.12s/it]\u001b[A\n",
            " 50%|█████     | 10/20 [00:49<00:49,  4.97s/it]\u001b[A\n",
            " 55%|█████▌    | 11/20 [00:53<00:43,  4.88s/it]\u001b[A\n",
            " 60%|██████    | 12/20 [00:58<00:39,  4.90s/it]\u001b[A\n",
            " 65%|██████▌   | 13/20 [01:03<00:34,  4.92s/it]\u001b[A\n",
            " 70%|███████   | 14/20 [01:08<00:29,  4.95s/it]\u001b[A\n",
            " 75%|███████▌  | 15/20 [01:13<00:24,  4.96s/it]\u001b[A\n",
            " 80%|████████  | 16/20 [01:18<00:19,  4.97s/it]\u001b[A\n",
            " 85%|████████▌ | 17/20 [01:23<00:14,  4.94s/it]\u001b[A\n",
            " 90%|█████████ | 18/20 [01:28<00:09,  4.94s/it]\u001b[A\n",
            " 95%|█████████▌| 19/20 [01:33<00:04,  4.93s/it]\u001b[A\n",
            "100%|██████████| 20/20 [01:38<00:00,  4.91s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgL5E9nVZ8tj",
        "outputId": "d24cf14e-f96d-4670-e878-bc65f0948b90"
      },
      "source": [
        "for i in list_metrics:\n",
        "    print('word: ', i)\n",
        "    print('_______')\n",
        "    print('pmi results: ')\n",
        "    for result in list_metrics[i][0][:5]:\n",
        "      print(' '.join(result))\n",
        "    print('_______')\n",
        "    print('log likelihood results: ')\n",
        "    for result in list_metrics[i][1][:5]:\n",
        "      print(' '.join(result))\n",
        "    print('_______')\n",
        "    print('dice results: ')\n",
        "    for result in list_metrics[i][2][:5]:\n",
        "      print(' '.join(result))\n",
        "    print('\\n\\n')"
      ],
      "id": "bgL5E9nVZ8tj",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word:  jane_iredale\n",
            "_______\n",
            "pmi results: \n",
            "jane_iredale purepressed\n",
            "jane_iredale pressed\n",
            "sells jane_iredale\n",
            "jane_iredale cosmetics\n",
            "jane_iredale pure\n",
            "_______\n",
            "log likelihood results: \n",
            "jane_iredale products\n",
            "love jane_iredale\n",
            "jane_iredale pressed\n",
            "jane_iredale purepressed\n",
            "jane_iredale website\n",
            "_______\n",
            "dice results: \n",
            "jane_iredale pressed\n",
            "jane_iredale purepressed\n",
            "jane_iredale website\n",
            "jane_iredale pure\n",
            "jane_iredale products\n",
            "\n",
            "\n",
            "\n",
            "word:  jack_black\n",
            "_______\n",
            "pmi results: \n",
            "jack_black industrial\n",
            "jack_black beard\n",
            "jack_black pomades\n",
            "\"> jack_black\n",
            "jack_black clay\n",
            "_______\n",
            "log likelihood results: \n",
            "jack_black products\n",
            "jack_black beard\n",
            "jack_black true\n",
            "\"> jack_black\n",
            "jack_black lip\n",
            "_______\n",
            "dice results: \n",
            "jack_black beard\n",
            "\"> jack_black\n",
            "jack_black true\n",
            "jack_black clay\n",
            "jack_black industrial\n",
            "\n",
            "\n",
            "\n",
            "word:  mario_badescu\n",
            "_______\n",
            "pmi results: \n",
            "mario_badescu enzyme\n",
            "mario_badescu buffering\n",
            "mario_badescu rose\n",
            "mario_badescu vitamin\n",
            "fan mario_badescu\n",
            "_______\n",
            "log likelihood results: \n",
            "mario_badescu products\n",
            "mario_badescu buffering\n",
            "mario_badescu enzyme\n",
            "mario_badescu drying\n",
            "fan mario_badescu\n",
            "_______\n",
            "dice results: \n",
            "mario_badescu buffering\n",
            "mario_badescu enzyme\n",
            "mario_badescu rose\n",
            "fan mario_badescu\n",
            "mario_badescu products\n",
            "\n",
            "\n",
            "\n",
            "word:  paul_mitchell\n",
            "_______\n",
            "pmi results: \n",
            "paul_mitchell freeze\n",
            "paul_mitchell awapuhi\n",
            "paul_mitchell sculpting\n",
            "paul_mitchell detangler\n",
            "paul_mitchell tea\n",
            "_______\n",
            "log likelihood results: \n",
            "paul_mitchell freeze\n",
            "paul_mitchell products\n",
            "paul_mitchell awapuhi\n",
            "paul_mitchell tea\n",
            "paul_mitchell detangler\n",
            "_______\n",
            "dice results: \n",
            "paul_mitchell freeze\n",
            "paul_mitchell awapuhi\n",
            "paul_mitchell detangler\n",
            "paul_mitchell tea\n",
            "paul_mitchell school\n",
            "\n",
            "\n",
            "\n",
            "word:  juice_beauty\n",
            "_______\n",
            "pmi results: \n",
            "juice_beauty stem\n",
            "juice_beauty green\n",
            "\"> juice_beauty\n",
            "juice_beauty cc\n",
            "impressed juice_beauty\n",
            "_______\n",
            "log likelihood results: \n",
            "juice_beauty green\n",
            "juice_beauty products\n",
            "juice_beauty stem\n",
            "\"> juice_beauty\n",
            "juice_beauty cc\n",
            "_______\n",
            "dice results: \n",
            "juice_beauty stem\n",
            "juice_beauty green\n",
            "\"> juice_beauty\n",
            "juice_beauty cc\n",
            "impressed juice_beauty\n",
            "\n",
            "\n",
            "\n",
            "word:  mason_pearson\n",
            "_______\n",
            "pmi results: \n",
            "mason_pearson brushes\n",
            "mason_pearson brush\n",
            "purchased mason_pearson\n",
            "love mason_pearson\n",
            "mason_pearson hair\n",
            "_______\n",
            "log likelihood results: \n",
            "mason_pearson brush\n",
            "mason_pearson brushes\n",
            "purchased mason_pearson\n",
            "love mason_pearson\n",
            "mason_pearson hair\n",
            "_______\n",
            "dice results: \n",
            "mason_pearson brushes\n",
            "mason_pearson brush\n",
            "purchased mason_pearson\n",
            "love mason_pearson\n",
            "mason_pearson hair\n",
            "\n",
            "\n",
            "\n",
            "word:  badger_hair\n",
            "_______\n",
            "pmi results: \n",
            "badger_hair brush\n",
            "soap badger_hair\n",
            "_______\n",
            "log likelihood results: \n",
            "badger_hair brush\n",
            "soap badger_hair\n",
            "_______\n",
            "dice results: \n",
            "badger_hair brush\n",
            "soap badger_hair\n",
            "\n",
            "\n",
            "\n",
            "word:  escali_badger_hair\n",
            "_______\n",
            "pmi results: \n",
            "; escali_badger_hair\n",
            "escali_badger_hair brush\n",
            "_______\n",
            "log likelihood results: \n",
            "; escali_badger_hair\n",
            "escali_badger_hair brush\n",
            "_______\n",
            "dice results: \n",
            "; escali_badger_hair\n",
            "escali_badger_hair brush\n",
            "\n",
            "\n",
            "\n",
            "word:  hot_tools\n",
            "_______\n",
            "pmi results: \n",
            "hot_tools 1-1/4\n",
            "hot_tools grips\n",
            "hot_tools marcel\n",
            "forgot hot_tools\n",
            "hot_tools curling\n",
            "_______\n",
            "log likelihood results: \n",
            "hot_tools curling\n",
            "hot_tools brand\n",
            "hot_tools products\n",
            "hot_tools professional\n",
            "love hot_tools\n",
            "_______\n",
            "dice results: \n",
            "hot_tools curling\n",
            "hot_tools marcel\n",
            "hot_tools professional\n",
            "hot_tools 1-1/4\n",
            "hot_tools brand\n",
            "\n",
            "\n",
            "\n",
            "word:  bare_minerals\n",
            "_______\n",
            "pmi results: \n",
            "bare_minerals powders\n",
            "compact bare_minerals\n",
            "switched bare_minerals\n",
            "bare_minerals complexion\n",
            "compared bare_minerals\n",
            "_______\n",
            "log likelihood results: \n",
            "bare_minerals powders\n",
            "compact bare_minerals\n",
            "bare_minerals powder\n",
            "switched bare_minerals\n",
            "bare_minerals complexion\n",
            "_______\n",
            "dice results: \n",
            "bare_minerals powders\n",
            "compact bare_minerals\n",
            "switched bare_minerals\n",
            "bare_minerals complexion\n",
            "compared bare_minerals\n",
            "\n",
            "\n",
            "\n",
            "word:  roche-posay\n",
            "_______\n",
            "pmi results: \n",
            "_______\n",
            "log likelihood results: \n",
            "_______\n",
            "dice results: \n",
            "\n",
            "\n",
            "\n",
            "word:  la_roche-posay\n",
            "_______\n",
            "pmi results: \n",
            "la_roche-posay anthelios\n",
            "la_roche-posay rosaliac\n",
            "la_roche-posay serozinc\n",
            "la_roche-posay toleriane\n",
            "\"> la_roche-posay\n",
            "_______\n",
            "log likelihood results: \n",
            "la_roche-posay rosaliac\n",
            "la_roche-posay anthelios\n",
            "la_roche-posay serozinc\n",
            "la_roche-posay toleriane\n",
            "la_roche-posay products\n",
            "_______\n",
            "dice results: \n",
            "la_roche-posay rosaliac\n",
            "la_roche-posay anthelios\n",
            "la_roche-posay serozinc\n",
            "la_roche-posay toleriane\n",
            "\"> la_roche-posay\n",
            "\n",
            "\n",
            "\n",
            "word:  billy_jealousy\n",
            "_______\n",
            "pmi results: \n",
            "\"> billy_jealousy\n",
            "happy billy_jealousy\n",
            "billy_jealousy products\n",
            ", billy_jealousy\n",
            "billy_jealousy ,\n",
            "_______\n",
            "log likelihood results: \n",
            "billy_jealousy products\n",
            "\"> billy_jealousy\n",
            "happy billy_jealousy\n",
            ", billy_jealousy\n",
            "billy_jealousy ,\n",
            "_______\n",
            "dice results: \n",
            "\"> billy_jealousy\n",
            "happy billy_jealousy\n",
            "billy_jealousy products\n",
            ", billy_jealousy\n",
            "billy_jealousy ,\n",
            "\n",
            "\n",
            "\n",
            "word:  anthony_logistics\n",
            "_______\n",
            "pmi results: \n",
            "\"> anthony_logistics\n",
            "anthony_logistics men\n",
            "fan anthony_logistics\n",
            "anthony_logistics products\n",
            "use anthony_logistics\n",
            "_______\n",
            "log likelihood results: \n",
            "\"> anthony_logistics\n",
            "anthony_logistics men\n",
            "anthony_logistics products\n",
            "fan anthony_logistics\n",
            "use anthony_logistics\n",
            "_______\n",
            "dice results: \n",
            "\"> anthony_logistics\n",
            "anthony_logistics men\n",
            "fan anthony_logistics\n",
            "anthony_logistics products\n",
            "use anthony_logistics\n",
            "\n",
            "\n",
            "\n",
            "word:  iredale_bb\n",
            "_______\n",
            "pmi results: \n",
            "_______\n",
            "log likelihood results: \n",
            "_______\n",
            "dice results: \n",
            "\n",
            "\n",
            "\n",
            "word:  jane_iredale_bb\n",
            "_______\n",
            "pmi results: \n",
            "jane_iredale_bb cream\n",
            "use jane_iredale_bb\n",
            "_______\n",
            "log likelihood results: \n",
            "jane_iredale_bb cream\n",
            "use jane_iredale_bb\n",
            "_______\n",
            "dice results: \n",
            "jane_iredale_bb cream\n",
            "use jane_iredale_bb\n",
            "\n",
            "\n",
            "\n",
            "word:  elizabeth_arden\n",
            "_______\n",
            "pmi results: \n",
            "directly elizabeth_arden\n",
            "wearing elizabeth_arden\n",
            "elizabeth_arden mascara\n",
            "purchased elizabeth_arden\n",
            "tried elizabeth_arden\n",
            "_______\n",
            "log likelihood results: \n",
            "wearing elizabeth_arden\n",
            "elizabeth_arden mascara\n",
            "used elizabeth_arden\n",
            "directly elizabeth_arden\n",
            "tried elizabeth_arden\n",
            "_______\n",
            "dice results: \n",
            "directly elizabeth_arden\n",
            "wearing elizabeth_arden\n",
            "elizabeth_arden mascara\n",
            "purchased elizabeth_arden\n",
            "tried elizabeth_arden\n",
            "\n",
            "\n",
            "\n",
            "word:  la_roche\n",
            "_______\n",
            "pmi results: \n",
            "la_roche posay\n",
            "using la_roche\n",
            "use la_roche\n",
            "la_roche products\n",
            "love la_roche\n",
            "_______\n",
            "log likelihood results: \n",
            "la_roche posay\n",
            "use la_roche\n",
            "using la_roche\n",
            "love la_roche\n",
            "la_roche products\n",
            "_______\n",
            "dice results: \n",
            "la_roche posay\n",
            "using la_roche\n",
            "use la_roche\n",
            "la_roche products\n",
            "love la_roche\n",
            "\n",
            "\n",
            "\n",
            "word:  molton_brown\n",
            "_______\n",
            "pmi results: \n",
            "molton_brown black\n",
            "molton_brown products\n",
            "love molton_brown\n",
            "/// molton_brown\n",
            "molton_brown .\n",
            "_______\n",
            "log likelihood results: \n",
            "molton_brown products\n",
            "molton_brown black\n",
            "love molton_brown\n",
            "/// molton_brown\n",
            "molton_brown .\n",
            "_______\n",
            "dice results: \n",
            "molton_brown black\n",
            "molton_brown products\n",
            "love molton_brown\n",
            "/// molton_brown\n",
            "molton_brown .\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CFr2jdbgrmp",
        "outputId": "ac1d3f1a-8432-454f-c5d8-2959cc26a885"
      },
      "source": [
        "chunks_new = []\n",
        "for chunk in tqdm(chunks_prep1):\n",
        "  for ne in nes_desc_prep:\n",
        "    ne_split = ne.split()\n",
        "    if ne in chunk:\n",
        "      chunk = chunk.replace(ne, '_'.join(ne_split))\n",
        "  chunks_new.append(chunk)"
      ],
      "id": "9CFr2jdbgrmp",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 558/558 [00:00<00:00, 2302.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIJZ7ajEWfOR",
        "outputId": "84eebca6-b897-4d74-df47-1f022223aac2"
      },
      "source": [
        "list_metrics_desc = {}\n",
        "\n",
        "for ner in tqdm(nes_desc_split):\n",
        "  bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "  creature_filter = lambda *w: ner not in w \n",
        "  finder = BigramCollocationFinder.from_words(' '.join(chunks_new).split())\n",
        "  finder.apply_freq_filter(2)\n",
        "  finder.apply_ngram_filter(creature_filter)\n",
        "  list_metrics_desc[ner] = (finder.nbest(bigram_measures.pmi, 15), finder.nbest(bigram_measures.likelihood_ratio, 15), finder.nbest(bigram_measures.dice, 15))"
      ],
      "id": "FIJZ7ajEWfOR",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:34<00:00,  7.75s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayCynMUELiFv",
        "outputId": "64337469-121b-4723-fdc4-047457791a99"
      },
      "source": [
        "for i in list_metrics_desc:\n",
        "    print('word: ', i)\n",
        "    print('_______')\n",
        "    print('pmi results: ')\n",
        "    for result in list_metrics_desc[i][0][:5]:\n",
        "      print(' '.join(result))\n",
        "    print('_______')\n",
        "    print('log likelihood results: ')\n",
        "    for result in list_metrics_desc[i][1][:5]:\n",
        "      print(' '.join(result))\n",
        "    print('_______')\n",
        "    print('dice results: ')\n",
        "    for result in list_metrics_desc[i][2][:5]:\n",
        "      print(' '.join(result))\n",
        "    print('\\n\\n')"
      ],
      "id": "ayCynMUELiFv",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word:  mario_badescu_products\n",
            "_______\n",
            "pmi results: \n",
            "other mario_badescu_products\n",
            "mario_badescu_products are\n",
            "all mario_badescu_products\n",
            "love mario_badescu_products\n",
            "of mario_badescu_products\n",
            "_______\n",
            "log likelihood results: \n",
            "other mario_badescu_products\n",
            "mario_badescu_products .\n",
            "mario_badescu_products and\n",
            "mario_badescu_products are\n",
            "all mario_badescu_products\n",
            "_______\n",
            "dice results: \n",
            "other mario_badescu_products\n",
            "mario_badescu_products are\n",
            "all mario_badescu_products\n",
            "love mario_badescu_products\n",
            "of mario_badescu_products\n",
            "\n",
            "\n",
            "\n",
            "word:  jane_iredale_products\n",
            "_______\n",
            "pmi results: \n",
            "jane_iredale_products are\n",
            "love jane_iredale_products\n",
            "using jane_iredale_products\n",
            "all jane_iredale_products\n",
            "like jane_iredale_products\n",
            "_______\n",
            "log likelihood results: \n",
            "love jane_iredale_products\n",
            "jane_iredale_products are\n",
            "using jane_iredale_products\n",
            "jane_iredale_products and\n",
            "all jane_iredale_products\n",
            "_______\n",
            "dice results: \n",
            "jane_iredale_products are\n",
            "love jane_iredale_products\n",
            "using jane_iredale_products\n",
            "all jane_iredale_products\n",
            "like jane_iredale_products\n",
            "\n",
            "\n",
            "\n",
            "word:  paul_mitchell_products\n",
            "_______\n",
            "pmi results: \n",
            "real paul_mitchell_products\n",
            "other paul_mitchell_products\n",
            "all paul_mitchell_products\n",
            "like paul_mitchell_products\n",
            "paul_mitchell_products in\n",
            "_______\n",
            "log likelihood results: \n",
            "other paul_mitchell_products\n",
            "real paul_mitchell_products\n",
            "paul_mitchell_products in\n",
            "all paul_mitchell_products\n",
            "like paul_mitchell_products\n",
            "_______\n",
            "dice results: \n",
            "real paul_mitchell_products\n",
            "other paul_mitchell_products\n",
            "all paul_mitchell_products\n",
            "like paul_mitchell_products\n",
            "paul_mitchell_products in\n",
            "\n",
            "\n",
            "\n",
            "word:  jack_black_products\n",
            "_______\n",
            "pmi results: \n",
            "all jack_black_products\n",
            "jack_black_products are\n",
            "love jack_black_products\n",
            "jack_black_products for\n",
            "jack_black_products ,\n",
            "_______\n",
            "log likelihood results: \n",
            "all jack_black_products\n",
            "love jack_black_products\n",
            "jack_black_products are\n",
            "jack_black_products ,\n",
            "jack_black_products and\n",
            "_______\n",
            "dice results: \n",
            "all jack_black_products\n",
            "jack_black_products are\n",
            "love jack_black_products\n",
            "jack_black_products for\n",
            "jack_black_products ,\n",
            "\n",
            "\n",
            "\n",
            "word:  badger_hair_brush\n",
            "_______\n",
            "pmi results: \n",
            "escali badger_hair_brush\n",
            "badger_hair_brush ;\n",
            "a badger_hair_brush\n",
            "my badger_hair_brush\n",
            "badger_hair_brush and\n",
            "_______\n",
            "log likelihood results: \n",
            "escali badger_hair_brush\n",
            "badger_hair_brush ;\n",
            "a badger_hair_brush\n",
            "badger_hair_brush and\n",
            "my badger_hair_brush\n",
            "_______\n",
            "dice results: \n",
            "escali badger_hair_brush\n",
            "badger_hair_brush ;\n",
            "a badger_hair_brush\n",
            "my badger_hair_brush\n",
            "badger_hair_brush and\n",
            "\n",
            "\n",
            "\n",
            "word:  escali_badger_hair_brush\n",
            "_______\n",
            "pmi results: \n",
            "_______\n",
            "log likelihood results: \n",
            "_______\n",
            "dice results: \n",
            "\n",
            "\n",
            "\n",
            "word:  juice_beauty_products\n",
            "_______\n",
            "pmi results: \n",
            "other juice_beauty_products\n",
            "juice_beauty_products for\n",
            "the juice_beauty_products\n",
            "juice_beauty_products ,\n",
            "juice_beauty_products and\n",
            "_______\n",
            "log likelihood results: \n",
            "other juice_beauty_products\n",
            "the juice_beauty_products\n",
            "juice_beauty_products ,\n",
            "juice_beauty_products and\n",
            "juice_beauty_products for\n",
            "_______\n",
            "dice results: \n",
            "other juice_beauty_products\n",
            "juice_beauty_products for\n",
            "the juice_beauty_products\n",
            "juice_beauty_products ,\n",
            "juice_beauty_products and\n",
            "\n",
            "\n",
            "\n",
            "word:  hot_tools_products\n",
            "_______\n",
            "pmi results: \n",
            "other hot_tools_products\n",
            "love hot_tools_products\n",
            "of hot_tools_products\n",
            "hot_tools_products !\n",
            "hot_tools_products ,\n",
            "_______\n",
            "log likelihood results: \n",
            "love hot_tools_products\n",
            "hot_tools_products ,\n",
            "other hot_tools_products\n",
            "of hot_tools_products\n",
            "my hot_tools_products\n",
            "_______\n",
            "dice results: \n",
            "other hot_tools_products\n",
            "love hot_tools_products\n",
            "of hot_tools_products\n",
            "hot_tools_products !\n",
            "hot_tools_products ,\n",
            "\n",
            "\n",
            "\n",
            "word:  mason_pearson_brush\n",
            "_______\n",
            "pmi results: \n",
            "mason_pearson_brush for\n",
            "my mason_pearson_brush\n",
            "the mason_pearson_brush\n",
            "mason_pearson_brush and\n",
            "mason_pearson_brush .\n",
            "_______\n",
            "log likelihood results: \n",
            "the mason_pearson_brush\n",
            "my mason_pearson_brush\n",
            "mason_pearson_brush for\n",
            "mason_pearson_brush .\n",
            "mason_pearson_brush and\n",
            "_______\n",
            "dice results: \n",
            "mason_pearson_brush for\n",
            "my mason_pearson_brush\n",
            "the mason_pearson_brush\n",
            "mason_pearson_brush and\n",
            "mason_pearson_brush .\n",
            "\n",
            "\n",
            "\n",
            "word:  paul_mitchell_product\n",
            "_______\n",
            "pmi results: \n",
            "any paul_mitchell_product\n",
            "a paul_mitchell_product\n",
            "paul_mitchell_product .\n",
            "_______\n",
            "log likelihood results: \n",
            "any paul_mitchell_product\n",
            "paul_mitchell_product .\n",
            "a paul_mitchell_product\n",
            "_______\n",
            "dice results: \n",
            "any paul_mitchell_product\n",
            "a paul_mitchell_product\n",
            "paul_mitchell_product .\n",
            "\n",
            "\n",
            "\n",
            "word:  jane_iredale_product\n",
            "_______\n",
            "pmi results: \n",
            "this jane_iredale_product\n",
            "jane_iredale_product .\n",
            "_______\n",
            "log likelihood results: \n",
            "this jane_iredale_product\n",
            "jane_iredale_product .\n",
            "_______\n",
            "dice results: \n",
            "this jane_iredale_product\n",
            "jane_iredale_product .\n",
            "\n",
            "\n",
            "\n",
            "word:  jack_black_product\n",
            "_______\n",
            "pmi results: \n",
            "jack_black_product line\n",
            "the jack_black_product\n",
            "jack_black_product is\n",
            "_______\n",
            "log likelihood results: \n",
            "the jack_black_product\n",
            "jack_black_product line\n",
            "jack_black_product is\n",
            "_______\n",
            "dice results: \n",
            "jack_black_product line\n",
            "the jack_black_product\n",
            "jack_black_product is\n",
            "\n",
            "\n",
            "\n",
            "word:  roche-posay_products\n",
            "_______\n",
            "pmi results: \n",
            "la roche-posay_products\n",
            "roche-posay_products for\n",
            "_______\n",
            "log likelihood results: \n",
            "la roche-posay_products\n",
            "roche-posay_products for\n",
            "_______\n",
            "dice results: \n",
            "la roche-posay_products\n",
            "roche-posay_products for\n",
            "\n",
            "\n",
            "\n",
            "word:  mason_pearson_brushes\n",
            "_______\n",
            "pmi results: \n",
            "mason_pearson_brushes for\n",
            "_______\n",
            "log likelihood results: \n",
            "mason_pearson_brushes for\n",
            "_______\n",
            "dice results: \n",
            "mason_pearson_brushes for\n",
            "\n",
            "\n",
            "\n",
            "word:  la_roche-posay_products\n",
            "_______\n",
            "pmi results: \n",
            "_______\n",
            "log likelihood results: \n",
            "_______\n",
            "dice results: \n",
            "\n",
            "\n",
            "\n",
            "word:  anthony_logistics_products\n",
            "_______\n",
            "pmi results: \n",
            "of anthony_logistics_products\n",
            "anthony_logistics_products for\n",
            "anthony_logistics_products ,\n",
            "_______\n",
            "log likelihood results: \n",
            "of anthony_logistics_products\n",
            "anthony_logistics_products for\n",
            "anthony_logistics_products ,\n",
            "_______\n",
            "dice results: \n",
            "of anthony_logistics_products\n",
            "anthony_logistics_products for\n",
            "anthony_logistics_products ,\n",
            "\n",
            "\n",
            "\n",
            "word:  jack_black_shampoo\n",
            "_______\n",
            "pmi results: \n",
            "the jack_black_shampoo\n",
            "_______\n",
            "log likelihood results: \n",
            "the jack_black_shampoo\n",
            "_______\n",
            "dice results: \n",
            "the jack_black_shampoo\n",
            "\n",
            "\n",
            "\n",
            "word:  bare_minerals_powders\n",
            "_______\n",
            "pmi results: \n",
            "compact bare_minerals_powders\n",
            "bare_minerals_powders .\n",
            "_______\n",
            "log likelihood results: \n",
            "compact bare_minerals_powders\n",
            "bare_minerals_powders .\n",
            "_______\n",
            "dice results: \n",
            "compact bare_minerals_powders\n",
            "bare_minerals_powders .\n",
            "\n",
            "\n",
            "\n",
            "word:  iredale_bb_cream\n",
            "_______\n",
            "pmi results: \n",
            "jane iredale_bb_cream\n",
            "_______\n",
            "log likelihood results: \n",
            "jane iredale_bb_cream\n",
            "_______\n",
            "dice results: \n",
            "jane iredale_bb_cream\n",
            "\n",
            "\n",
            "\n",
            "word:  jane_iredale_bb_cream\n",
            "_______\n",
            "pmi results: \n",
            "_______\n",
            "log likelihood results: \n",
            "_______\n",
            "dice results: \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG4qB_1Rnb-t"
      },
      "source": [
        "Столкнулись с очевидной проблемой. Когда смотрим на сущности без дескрипторов, в коллокации постоянно влезают дескрипторы, которые для отчета о пользовательском восприятии товара будет совсем неинформативны. Когда смотрим на сущности с дескрипторами, вариантов получается слишком мало -- мало кто из пользователей будет писать полное название продукта, скорее ограничится только сущностью или только дескриптором. Здесь как раз может помочь бонусное задание -- создать словарь, в котором каждому продукту/бренду будет сопоставлены его возможные наименования: просто сущности, дескрипторы, сущности+дескрипторы -- и оценивать коллокации со всеми возможными упоминаниями товара. Я оценю лучшую метрику после того, как сделаю бонусное. И тогда же посмотрю триграммы."
      ],
      "id": "KG4qB_1Rnb-t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ_tAUCGijPT"
      },
      "source": [
        "nes_desc_prep.sort(key=lambda s: len(s.split()), reverse=True)\n",
        "nes_prep.sort(key=lambda s: len(s.split()), reverse=True)"
      ],
      "id": "FQ_tAUCGijPT",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR-sEkx_pDpf"
      },
      "source": [
        "Даже внутри нашего маленького списка лучших сущности повторяются (la roche-posay, roche-posay). Сначала разберемся с этим. Создаем словарь, где ключом будет самое краткое, но при этом полное название бренда. Значениями будут все возможные варианты названий, включая ключ."
      ],
      "id": "nR-sEkx_pDpf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMPm5XPIpGoZ"
      },
      "source": [
        "list_remove = []\n",
        "dict_variants = {}"
      ],
      "id": "AMPm5XPIpGoZ",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnxST0PVpQ_B",
        "outputId": "ba49a320-0e40-4a37-f64e-ac24933127c4"
      },
      "source": [
        "for item_1 in nes_prep:\n",
        "  for item_2 in nes_prep:\n",
        "    if item_1 in item_2 and item_1 != item_2:\n",
        "      dict_variants[item_2] = [item_1]\n",
        "      list_remove.append(item_1)\n",
        "rem_keys = set(nes_prep) - set(list_remove)\n",
        "print(dict_variants)"
      ],
      "id": "EnxST0PVpQ_B",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'jane iredale bb': ['jane iredale'], 'escali badger hair': ['badger hair'], 'la roche-posay': ['roche-posay']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3KfOosMpUId",
        "outputId": "305e57bb-c253-407f-a845-6e8845e58208"
      },
      "source": [
        "for item_1 in rem_keys:\n",
        "  for item_2 in rem_keys:\n",
        "    if item_1 in item_2:\n",
        "      if item_1 in dict_variants:\n",
        "        dict_variants[item_1].append(item_2)\n",
        "      else:\n",
        "        dict_variants[item_1] = [item_2]\n",
        "print(dict_variants)"
      ],
      "id": "M3KfOosMpUId",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'jane iredale bb': ['jane iredale', 'jane iredale bb'], 'escali badger hair': ['badger hair', 'escali badger hair'], 'la roche-posay': ['roche-posay', 'la roche-posay'], 'elizabeth arden': ['elizabeth arden'], 'anthony logistics': ['anthony logistics'], 'molton brown': ['molton brown'], 'hot tools': ['hot tools'], 'mario badescu': ['mario badescu'], 'mason pearson': ['mason pearson'], 'juice beauty': ['juice beauty'], 'billy jealousy': ['billy jealousy'], 'bare minerals': ['bare minerals'], 'jack black': ['jack black'], 'paul mitchell': ['paul mitchell']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-sy6nlEppW1"
      },
      "source": [
        "Теперь ищем варианты названий по нашим полным спискам сущностей+дескрипторов."
      ],
      "id": "D-sy6nlEppW1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTDLwjvvUT_I"
      },
      "source": [
        "nes_desc_proc_all = [preprocess_text(x) for x in nes_desc]\n",
        "nes_proc_all = [preprocess_text(x) for x in nes]"
      ],
      "id": "lTDLwjvvUT_I",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCjXef4LrP7p"
      },
      "source": [
        "for key in dict_variants.keys():\n",
        "  for ne in nes_desc_proc_all:\n",
        "    if key in ne:\n",
        "      dict_variants[key].append(ne)\n",
        "      dict_variants[key].append(' '.join(ne.replace(key, '').split()))\n",
        "  for ne in nes_proc_all:\n",
        "    if key in ne:\n",
        "      dict_variants[key].append(ne)\n",
        "      dict_variants[key].append(' '.join(ne.replace(key, '').split()))"
      ],
      "id": "dCjXef4LrP7p",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxbFY-GHpxUs"
      },
      "source": [
        "Вероятно будут повторы, сразу от них избавимся."
      ],
      "id": "SxbFY-GHpxUs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j76DiJRzu_1S",
        "outputId": "c78f3eaa-8041-4d54-fde0-7f49d62f4fe1"
      },
      "source": [
        "for key in tqdm(dict_variants.keys()):\n",
        "  dict_variants[key] = set(dict_variants[key])"
      ],
      "id": "j76DiJRzu_1S",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 18151.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIKi0rg4p1CJ"
      },
      "source": [
        "Смотрим, что вышло."
      ],
      "id": "zIKi0rg4p1CJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vug9Ty_MweH7",
        "outputId": "8e44807f-eb77-4b21-c37a-f506e7188b4c"
      },
      "source": [
        "dict_variants"
      ],
      "id": "Vug9Ty_MweH7",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anthony logistics': {'',\n",
              "  'anthony logistics',\n",
              "  'anthony logistics products',\n",
              "  'products'},\n",
              " 'bare minerals': {'',\n",
              "  'bare minerals',\n",
              "  'bare minerals brushes',\n",
              "  'bare minerals foundation',\n",
              "  'bare minerals powder',\n",
              "  'bare minerals powder foundation',\n",
              "  'bare minerals powders',\n",
              "  'bare minerals product',\n",
              "  'bare minerals products',\n",
              "  'bare minerals ready',\n",
              "  'bare minerals ready foundation',\n",
              "  'brushes',\n",
              "  'foundation',\n",
              "  'powder',\n",
              "  'powder foundation',\n",
              "  'powders',\n",
              "  'product',\n",
              "  'products',\n",
              "  'ready',\n",
              "  'ready foundation'},\n",
              " 'billy jealousy': {'',\n",
              "  'billy jealousy',\n",
              "  'billy jealousy moisturizer',\n",
              "  'billy jealousy product',\n",
              "  'billy jealousy products',\n",
              "  'moisturizer',\n",
              "  'product',\n",
              "  'products'},\n",
              " 'elizabeth arden': {'',\n",
              "  'elizabeth arden',\n",
              "  'elizabeth arden mascara',\n",
              "  'elizabeth arden perfume',\n",
              "  'elizabeth arden product',\n",
              "  'elizabeth arden products',\n",
              "  'elizabeth arden sunflowers',\n",
              "  'elizabeth arden sunflowers perfume',\n",
              "  'mascara',\n",
              "  'perfume',\n",
              "  'product',\n",
              "  'products',\n",
              "  'sunflowers',\n",
              "  'sunflowers perfume'},\n",
              " 'escali badger hair': {'',\n",
              "  'badger hair',\n",
              "  'brush',\n",
              "  'escali badger hair',\n",
              "  'escali badger hair brush'},\n",
              " 'hot tools': {'',\n",
              "  'hot tools',\n",
              "  'hot tools product',\n",
              "  'hot tools products',\n",
              "  'product',\n",
              "  'products'},\n",
              " 'jack black': {'',\n",
              "  'beard lube conditioning',\n",
              "  'beard lube conditioning cream',\n",
              "  'body wash',\n",
              "  'cream',\n",
              "  'daily',\n",
              "  'daily moisturizer',\n",
              "  'epic moisture',\n",
              "  'epic moisture lotion',\n",
              "  'face lotion',\n",
              "  'jack black',\n",
              "  'jack black beard lube conditioning',\n",
              "  'jack black beard lube conditioning cream',\n",
              "  'jack black body wash',\n",
              "  'jack black cream',\n",
              "  'jack black daily',\n",
              "  'jack black daily moisturizer',\n",
              "  'jack black epic moisture',\n",
              "  'jack black epic moisture lotion',\n",
              "  'jack black face lotion',\n",
              "  'jack black lip balm',\n",
              "  'jack black lip balms',\n",
              "  'jack black lube conditioning shave',\n",
              "  'jack black lube conditioning shave product',\n",
              "  'jack black product',\n",
              "  'jack black products',\n",
              "  'jack black shampoo',\n",
              "  'jack black true volume',\n",
              "  'jack black true volume shampoo',\n",
              "  'jack black true volume thickening',\n",
              "  'jack black true volume thickening shampoo',\n",
              "  'jack black wax',\n",
              "  'jack blacks',\n",
              "  'jack blacks products',\n",
              "  'lip balm',\n",
              "  'lip balms',\n",
              "  'lube conditioning shave',\n",
              "  'lube conditioning shave product',\n",
              "  'product',\n",
              "  'products',\n",
              "  's',\n",
              "  's products',\n",
              "  'shampoo',\n",
              "  'true volume',\n",
              "  'true volume shampoo',\n",
              "  'true volume thickening',\n",
              "  'true volume thickening shampoo',\n",
              "  'wax'},\n",
              " 'jane iredale bb': {'',\n",
              "  'cream',\n",
              "  'foundation',\n",
              "  'jane iredale',\n",
              "  'jane iredale bb',\n",
              "  'jane iredale bb cream',\n",
              "  'jane iredale bb foundation'},\n",
              " 'juice beauty': {'',\n",
              "  'age defy solutions',\n",
              "  'age defy solutions moisturizer',\n",
              "  'foundation',\n",
              "  'juice beauty',\n",
              "  'juice beauty age defy solutions',\n",
              "  'juice beauty age defy solutions moisturizer',\n",
              "  'juice beauty foundation',\n",
              "  'juice beauty moisturizer',\n",
              "  'juice beauty moisturizers',\n",
              "  'juice beauty product',\n",
              "  'juice beauty products',\n",
              "  'moisturizer',\n",
              "  'moisturizers',\n",
              "  'product',\n",
              "  'products'},\n",
              " 'la roche-posay': {'',\n",
              "  'anthelios sx',\n",
              "  'anthelios sx sunscreen',\n",
              "  'foundation',\n",
              "  'la roche-posay',\n",
              "  'la roche-posay anthelios sx',\n",
              "  'la roche-posay anthelios sx sunscreen',\n",
              "  'la roche-posay foundation',\n",
              "  'la roche-posay moisturizer',\n",
              "  'la roche-posay products',\n",
              "  'la roche-posay sunscreens',\n",
              "  'moisturizer',\n",
              "  'products',\n",
              "  'roche-posay',\n",
              "  'sunscreens'},\n",
              " 'mario badescu': {'',\n",
              "  'chamomile cleansing',\n",
              "  'chamomile cleansing lotion',\n",
              "  'drying',\n",
              "  'drying lotion',\n",
              "  'love',\n",
              "  'love mario badescu',\n",
              "  'love mario badescu products',\n",
              "  'love products',\n",
              "  'mario badescu',\n",
              "  'mario badescu chamomile cleansing',\n",
              "  'mario badescu chamomile cleansing lotion',\n",
              "  'mario badescu drying',\n",
              "  'mario badescu drying lotion',\n",
              "  'mario badescu moisturizer',\n",
              "  'mario badescu product',\n",
              "  'mario badescu products',\n",
              "  'mario badescu rose',\n",
              "  'mario badescu rose water',\n",
              "  'mario badescu skin care products',\n",
              "  'mario badescu spray',\n",
              "  'moisturizer',\n",
              "  'product',\n",
              "  'product mario badescu',\n",
              "  'product mario badescu rose',\n",
              "  'product mario badescu rose water',\n",
              "  'product rose',\n",
              "  'product rose water',\n",
              "  'products',\n",
              "  'rose',\n",
              "  'rose water',\n",
              "  'skin care products',\n",
              "  'spray'},\n",
              " 'mason pearson': {'',\n",
              "  'brush',\n",
              "  'brushes',\n",
              "  'friends',\n",
              "  'friends brush',\n",
              "  'friends mason pearson',\n",
              "  'friends mason pearson brush',\n",
              "  'handy mixed dark ruby bristle',\n",
              "  'handy mixed dark ruby bristle brush',\n",
              "  'mason pearson',\n",
              "  'mason pearson brush',\n",
              "  'mason pearson brushes',\n",
              "  'mason pearson handy mixed dark ruby bristle',\n",
              "  'mason pearson handy mixed dark ruby bristle brush',\n",
              "  'mason pearson products',\n",
              "  'products',\n",
              "  'size',\n",
              "  'size brush',\n",
              "  'size mason pearson',\n",
              "  'size mason pearson brush'},\n",
              " 'molton brown': {'',\n",
              "  'black pepper',\n",
              "  'black pepper products',\n",
              "  'body wash',\n",
              "  'molton brown',\n",
              "  'molton brown black pepper',\n",
              "  'molton brown black pepper products',\n",
              "  'molton brown body wash',\n",
              "  'molton brown pink pepperpod',\n",
              "  'molton brown pink pepperpod body lotion',\n",
              "  'molton brown products',\n",
              "  'pink pepperpod',\n",
              "  'pink pepperpod body lotion',\n",
              "  'products'},\n",
              " 'paul mitchell': {'',\n",
              "  'awapuhi',\n",
              "  'awapuhi shampoo',\n",
              "  'conditioner',\n",
              "  'conditioner paul mitchell',\n",
              "  'extra body daily boost',\n",
              "  'extra body daily boost spray',\n",
              "  'gels',\n",
              "  'keratin',\n",
              "  'keratin product',\n",
              "  'men',\n",
              "  'men product',\n",
              "  'paul mitchell',\n",
              "  'paul mitchell awapuhi',\n",
              "  'paul mitchell awapuhi shampoo',\n",
              "  'paul mitchell conditioner',\n",
              "  'paul mitchell extra body daily boost',\n",
              "  'paul mitchell extra body daily boost spray',\n",
              "  'paul mitchell gels',\n",
              "  'paul mitchell keratin',\n",
              "  'paul mitchell keratin product',\n",
              "  'paul mitchell men',\n",
              "  'paul mitchell men product',\n",
              "  'paul mitchell product',\n",
              "  'paul mitchell products',\n",
              "  'paul mitchell shampoo',\n",
              "  'paul mitchell spray',\n",
              "  'paul mitchell tea tree',\n",
              "  'paul mitchell tea tree shampoo',\n",
              "  'product',\n",
              "  'products',\n",
              "  'real',\n",
              "  'real \"',\n",
              "  'real \" paul mitchell',\n",
              "  'real \" paul mitchell shampoo',\n",
              "  'real \" shampoo',\n",
              "  'real paul mitchell',\n",
              "  'real paul mitchell products',\n",
              "  'real products',\n",
              "  'shampoo',\n",
              "  'spray',\n",
              "  'tea tree',\n",
              "  'tea tree shampoo'}}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZE5rNQKtzLV"
      },
      "source": [
        "named_products = []\n",
        "no_name_products = []\n",
        "dict_variants['jack black'] = [x for x in dict_variants['jack black'] if len(x) > 2]\n",
        "for value in dict_variants['jack black']:\n",
        "  if 'jack black' in value:\n",
        "    named_products.append(value)\n",
        "  else:\n",
        "    no_name_products.append(value)"
      ],
      "id": "OZE5rNQKtzLV",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBH2zoF9rxvx"
      },
      "source": [
        "named_products.sort(key=lambda s: len(s.split()), reverse=True)"
      ],
      "id": "SBH2zoF9rxvx",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lR4DqIUsgOX"
      },
      "source": [
        "no_name_products.sort(key=lambda s: len(s.split()), reverse=True)"
      ],
      "id": "9lR4DqIUsgOX",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9AMKmwRse9P",
        "outputId": "b1a319d4-c0b4-4989-e7b9-22ff76f6e9be"
      },
      "source": [
        "named_products"
      ],
      "id": "F9AMKmwRse9P",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jack black beard lube conditioning cream',\n",
              " 'jack black lube conditioning shave product',\n",
              " 'jack black true volume thickening shampoo',\n",
              " 'jack black true volume thickening',\n",
              " 'jack black beard lube conditioning',\n",
              " 'jack black lube conditioning shave',\n",
              " 'jack black true volume shampoo',\n",
              " 'jack black epic moisture lotion',\n",
              " 'jack black face lotion',\n",
              " 'jack black epic moisture',\n",
              " 'jack black lip balms',\n",
              " 'jack black true volume',\n",
              " 'jack black daily moisturizer',\n",
              " 'jack black body wash',\n",
              " 'jack black lip balm',\n",
              " 'jack black wax',\n",
              " 'jack black cream',\n",
              " 'jack black daily',\n",
              " 'jack black shampoo',\n",
              " 'jack black product',\n",
              " 'jack blacks products',\n",
              " 'jack black products',\n",
              " 'jack blacks',\n",
              " 'jack black']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIisWcKIsok1",
        "outputId": "1798ff95-aa83-4b70-f126-62e77b508d55"
      },
      "source": [
        "no_name_products"
      ],
      "id": "QIisWcKIsok1",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beard lube conditioning cream',\n",
              " 'lube conditioning shave product',\n",
              " 'true volume thickening shampoo',\n",
              " 'lube conditioning shave',\n",
              " 'true volume shampoo',\n",
              " 'beard lube conditioning',\n",
              " 'true volume thickening',\n",
              " 'epic moisture lotion',\n",
              " 'daily moisturizer',\n",
              " 'lip balm',\n",
              " 'epic moisture',\n",
              " 'true volume',\n",
              " 's products',\n",
              " 'face lotion',\n",
              " 'lip balms',\n",
              " 'body wash',\n",
              " 'daily',\n",
              " 'cream',\n",
              " 'wax',\n",
              " 'products',\n",
              " 'product',\n",
              " 'shampoo']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgIn_sfX-Z5B"
      },
      "source": [
        "Покажу, как мы можем использовать эту группировку по названию бренда в случае если нам нжуно найти все нграммы с упоминанием бренда. Заменяем все возможные названия продуктов с именем бренда на флажок **jack_black_brand** , затем названия без упоминания (из второго списка) заменяем только в том случае, если они встречаются в одном комментарии с названиями из первого списка."
      ],
      "id": "NgIn_sfX-Z5B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFPYIUQfsqQ0",
        "outputId": "c051b942-5401-41ce-b34c-98023e28f606"
      },
      "source": [
        "jack_sents = []\n",
        "for chunk in tqdm(chunks_prep1):\n",
        "  sents = chunk.split('///')\n",
        "  for sent in sents:\n",
        "    for product in named_products:\n",
        "      if product in sent:\n",
        "        jack_sents.append(sent.replace(product, 'jack_black_brand'))\n",
        "jack_sents = list(set(jack_sents))"
      ],
      "id": "zFPYIUQfsqQ0",
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/558 [00:00<?, ?it/s]\u001b[A\n",
            " 30%|███       | 168/558 [00:00<00:00, 1675.34it/s]\u001b[A\n",
            " 60%|██████    | 336/558 [00:00<00:00, 1575.32it/s]\u001b[A\n",
            "100%|██████████| 558/558 [00:00<00:00, 1623.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EARUhNdf72t"
      },
      "source": [
        "jack_sents_new = []\n",
        "for sent in jack_sents:\n",
        "  for product in no_name_products:\n",
        "    if product in sent:\n",
        "      jack_sents_new.append(sent.replace(product, 'jack_black_brand'))\n",
        "    else:\n",
        "      jack_sents_new.append(sent)\n",
        "jack_sents_new = list(set(jack_sents_new))"
      ],
      "id": "-EARUhNdf72t",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlRqgnTipjjx"
      },
      "source": [
        "list_metrics_jack= {}\n",
        "\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "creature_filter = lambda *w: 'jack_black_brand' not in w \n",
        "finder = BigramCollocationFinder.from_words(' '.join(jack_sents_new).split())\n",
        "finder.apply_freq_filter(5)\n",
        "finder.apply_ngram_filter(creature_filter)\n",
        "list_metrics_jack['jack_black'] = (finder.nbest(bigram_measures.pmi, 15), finder.nbest(bigram_measures.likelihood_ratio, 15), finder.nbest(bigram_measures.jaccard, 15))"
      ],
      "id": "DlRqgnTipjjx",
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV8nUmFm7xbH",
        "outputId": "7fcd3df7-8b19-402b-9e5e-7c287a4badf3"
      },
      "source": [
        "print('pmi: ')\n",
        "for gram in list_metrics_jack['jack_black'][0]:\n",
        "  print(gram)\n",
        "print('\\nlikelihood_ratio: ')\n",
        "for gram in list_metrics_jack['jack_black'][1]:\n",
        "  print(gram)\n",
        "print('\\njaccard: ')\n",
        "for gram in list_metrics_jack['jack_black'][2]:\n",
        "  print(gram)"
      ],
      "id": "RV8nUmFm7xbH",
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pmi: \n",
            "('jack_black_brand', 'sleek')\n",
            "('conjunction', 'jack_black_brand')\n",
            "('jack_black_brand', 'industrial')\n",
            "('\">', 'jack_black_brand')\n",
            "('stick', 'jack_black_brand')\n",
            "('jack_black_brand', 'beard')\n",
            "('experience', 'jack_black_brand')\n",
            "('fan', 'jack_black_brand')\n",
            "('jack_black_brand', 'turned')\n",
            "('jack_black_brand', 'easier')\n",
            "('jack_black_brand', 'clay')\n",
            "('jack_black_brand', 'customer')\n",
            "('love', 'jack_black_brand')\n",
            "('impressive', 'jack_black_brand')\n",
            "('jack_black_brand', 'true')\n",
            "\n",
            "likelihood_ratio: \n",
            "('love', 'jack_black_brand')\n",
            "('\">', 'jack_black_brand')\n",
            "('using', 'jack_black_brand')\n",
            "('conjunction', 'jack_black_brand')\n",
            "('jack_black_brand', 'beard')\n",
            "('jack_black_brand', 'true')\n",
            "('jack_black_brand', 'products')\n",
            "('fan', 'jack_black_brand')\n",
            "('jack_black_brand', 'thickening')\n",
            "('jack_black_brand', 'line')\n",
            "('jack_black_brand', 'jack_black_brand')\n",
            "('jack_black_brand', 'shampoo')\n",
            "('like', 'jack_black_brand')\n",
            "('jack_black_brand', 'turned')\n",
            "('jack_black_brand', 'jack_black_brands')\n",
            "\n",
            "jaccard: \n",
            "('using', 'jack_black_brand')\n",
            "('jack_black_brand', 'jack_black_brand')\n",
            "('love', 'jack_black_brand')\n",
            "('.', 'jack_black_brand')\n",
            "('jack_black_brand', 'shampoo')\n",
            "('like', 'jack_black_brand')\n",
            "('jack_black_brand', ',')\n",
            "('jack_black_brand', '.')\n",
            "('jack_black_brand', 'products')\n",
            "('jack_black_brand', 'beard')\n",
            "('\">', 'jack_black_brand')\n",
            "('jack_black_brand', 'thickening')\n",
            "('jack_black_brand', 'true')\n",
            "('jack_black_brand', 'jack_black_brands')\n",
            "('jack_black_brand', 'product')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8FmmYGQ-i0i"
      },
      "source": [
        "list_metrics_jack= {}\n",
        "\n",
        "bigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "creature_filter = lambda *w: 'jack_black_brand' not in w \n",
        "finder = TrigramCollocationFinder.from_words(' '.join(jack_sents_new).split())\n",
        "finder.apply_freq_filter(5)\n",
        "finder.apply_ngram_filter(creature_filter)\n",
        "list_metrics_jack['jack_black_tri'] = (finder.nbest(bigram_measures.pmi, 15), finder.nbest(bigram_measures.likelihood_ratio, 15), finder.nbest(bigram_measures.jaccard, 15))"
      ],
      "id": "Z8FmmYGQ-i0i",
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEN37I-M-wMY",
        "outputId": "e8202b67-7e11-4480-ee16-7d27d94ba22f"
      },
      "source": [
        "print('pmi: ')\n",
        "for gram in list_metrics_jack['jack_black_tri'][0]:\n",
        "  print(gram)\n",
        "print('\\nlikelihood_ratio: ')\n",
        "for gram in list_metrics_jack['jack_black_tri'][1]:\n",
        "  print(gram)\n",
        "print('\\njaccard: ')\n",
        "for gram in list_metrics_jack['jack_black_tri'][2]:\n",
        "  print(gram)"
      ],
      "id": "DEN37I-M-wMY",
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pmi: \n",
            "('jack_black_brand', 'industrial', 'strength')\n",
            "('jack_black_brand', 'sleek', 'finish')\n",
            "('pairs', 'nicely', 'jack_black_brand')\n",
            "('jack_black_brand', 'past', 'compare')\n",
            "(\"i'll\", 'stick', 'jack_black_brand')\n",
            "('jack_black_brand', 'shea', 'butter')\n",
            "('originally', 'bought', 'jack_black_brand')\n",
            "('gave', 'sample', 'jack_black_brand')\n",
            "('utf8', '\">', 'jack_black_brand')\n",
            "('jack_black_brand', 'beard', 'lube')\n",
            "('big', 'fan', 'jack_black_brand')\n",
            "('jack_black_brand', 'intense', 'therapy')\n",
            "('designer', 'wish', 'jack_black_brand')\n",
            "('often', 'purchase', 'jack_black_brand')\n",
            "('think', 'jack_black_brand', 'easier')\n",
            "\n",
            "likelihood_ratio: \n",
            "('jack', 'black', 'jack_black_brand')\n",
            "('utf8', '\">', 'jack_black_brand')\n",
            "('jack_black_brand', 'true', 'volume')\n",
            "('jack_black_brand', 'lip', 'balm')\n",
            "('jack_black_brand', 'beard', 'lube')\n",
            "(\"i've\", 'using', 'jack_black_brand')\n",
            "('jack_black_brand', 'expansion', 'technology')\n",
            "('jack_black_brand', 'thickening', 'shampoo')\n",
            "('volume', 'thickening', 'jack_black_brand')\n",
            "('jack_black_brand', '<', '/a')\n",
            "('things', 'like', 'jack_black_brand')\n",
            "('.', 'love', 'jack_black_brand')\n",
            "('enjoyed', 'using', 'jack_black_brand')\n",
            "('using', 'conjunction', 'jack_black_brand')\n",
            "('using', 'jack_black_brand', 'seems')\n",
            "\n",
            "jaccard: \n",
            "('utf8', '\">', 'jack_black_brand')\n",
            "('jack_black_brand', 'true', 'volume')\n",
            "('jack_black_brand', 'beard', 'lube')\n",
            "('jack_black_brand', 'thickening', 'shampoo')\n",
            "('using', 'conjunction', 'jack_black_brand')\n",
            "('much', 'tried', 'jack_black_brand')\n",
            "('variety', 'shampoos', 'jack_black_brand')\n",
            "('jack', 'black', 'jack_black_brand')\n",
            "('really', 'like', 'jack_black_brand')\n",
            "('big', 'fan', 'jack_black_brand')\n",
            "('enjoyed', 'using', 'jack_black_brand')\n",
            "('volume', 'thickening', 'jack_black_brand')\n",
            "('jack_black_brand', 'intense', 'therapy')\n",
            "(\"i've\", 'using', 'jack_black_brand')\n",
            "('jack_black_brand', 'lathers', 'well')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aeithEr9168"
      },
      "source": [
        "Поскольку теперь у нас очень много результатов и следовтаельно нграмм, мы повысили значение частотного фильтра до 5. Судя по  результатам, лучше всего нам подойдет метрика PMI -- в jaccard мы видим огромное количество мусора и странных вхождений типа ('jack_black_brand', 'jack_black_brand'), он переоценивает всякий мусор типа запятых -- от мусора, конечно, можно было бы почистить корпус перед поиском биграмм, но все же странно оценивать его так высоко. То же в меньшей степени касается и log_likelihood ratio. PMI при это странные символы и знаки препинания не помешали выдать верные информативные нграммы: ('big', 'fan', 'jack_black_brand'), (\"i'll\", 'stick', 'jack_black_brand'), ('often', 'purchase', 'jack_black_brand')."
      ],
      "id": "2aeithEr9168"
    }
  ]
}