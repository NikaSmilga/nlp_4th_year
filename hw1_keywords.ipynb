{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4db39111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "import RAKE\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from summa import keywords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "stop = stopwords.words('russian')\n",
    "stop = stop + [\"это\", \"наш\", \"который\", \"всё\", \"её\"]\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3cacb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        lemmas.append(\n",
    "            m.parse(t)[0].normal_form\n",
    "        )\n",
    "    return ' '.join([x for x in lemmas if x not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0231e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rake(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        lemmas.append(\n",
    "            m.parse(t)[0].normal_form\n",
    "        )\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8398d6",
   "metadata": {},
   "source": [
    "### задания 1-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71668940",
   "metadata": {},
   "source": [
    "Для корпуса я взяла пять рандомных статей с Хабра. Под статьей на Хабре указываются теги, их я и использовалась в качестве ключевых слов. Свои теги я разметила прямо в txt-документах статей. Мой корпус хранится как список кортежей, где первый элемент -- текст, второй -- мои теги, третий -- исходные теги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2619e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "corpus = []\n",
    "for filename in os.listdir('./hw1'):\n",
    "    if 'txt' in filename:\n",
    "        with open('./hw1/' + filename) as a:\n",
    "            text = a.read()\n",
    "        text = text.replace('\\n', '').split('Теги: ')\n",
    "        corpus.append((text[0], [normalize_text(x) for x in text[1].split(', ')], [normalize_text(x) for x in text[2].split(', ')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36cd192",
   "metadata": {},
   "source": [
    "Оценим пересечение исходных тегов и тегов, проставленных мной. Я приведу количество совпадений и долю совпавших тегов (считаю ее как кол-во совпавших / объединение)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c04e1666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество общих keywords в документе:  3\n",
      "Доля общих keywords в документе:  0.3\n",
      "____________________________________\n",
      "Количество общих keywords в документе:  4\n",
      "Доля общих keywords в документе:  0.5\n",
      "____________________________________\n",
      "Количество общих keywords в документе:  2\n",
      "Доля общих keywords в документе:  0.2\n",
      "____________________________________\n",
      "Количество общих keywords в документе:  3\n",
      "Доля общих keywords в документе:  0.25\n",
      "____________________________________\n",
      "Количество общих keywords в документе:  3\n",
      "Доля общих keywords в документе:  0.21428571428571427\n",
      "____________________________________\n",
      "Среднее количество общих keywords:  3.0\n",
      "Доля общих keywords:  0.2777777777777778\n"
     ]
    }
   ],
   "source": [
    "count_global = 0\n",
    "len_global = 0\n",
    "for item in corpus:\n",
    "    len_keywords = len(item[1]) + len(item[2])\n",
    "    count = 0\n",
    "    for word in item[1]:\n",
    "        if word in item[2]:\n",
    "            count += 1\n",
    "            len_keywords -= 1\n",
    "    print(f\"Количество общих keywords в документе: \", count)\n",
    "    print(f\"Доля общих keywords в документе: \", count/len_keywords)\n",
    "    print(\"____________________________________\")\n",
    "    count_global += count\n",
    "    len_global += len_keywords\n",
    "print(f\"Среднее количество общих keywords: \", count_global/5)\n",
    "print(f\"Доля общих keywords: \", count_global/len_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd791d",
   "metadata": {},
   "source": [
    "Мы видим, что в среднем у нас получается три совпавших тега, доля совпадений -- 27%. Мне кажется, 3 тегов мало для интересных и насыщенных текстов с Хабра. Кроме того, я посмотрела теги Хабра, и они 1) вполне разумные, хоть и следуют немного иной логике, чем мои 2) присутствуют в текстах. Поэтому я буду использовать объединение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc5f1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_keywords = ([(x[0], list(set(x[1]).union(x[2]))) for x in corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5283fa7",
   "metadata": {},
   "source": [
    "### задание 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "413da715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_tfidf(text):\n",
    "    tf_idf_vector = vectorizer.transform([text])\n",
    "    feature_array = np.array(vectorizer.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(tf_idf_vector.toarray()).flatten()[::-1]\n",
    "    top20 = feature_array[tfidf_sorting][:20]\n",
    "    return list(top20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d42cccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake = RAKE.Rake(stop)\n",
    "rake_res = []\n",
    "for text in corpus_keywords:\n",
    "    rake_text = rake.run(normalize_rake(text[0]), maxWords=3, minFrequency=2)\n",
    "    rake_res.append([x[0] for x in rake_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "87f99e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_res = []\n",
    "for text in corpus_keywords:\n",
    "    tr_text = keywords.keywords(normalize_text(text[0]), language='russian', additional_stopwords=stop, scores=True)\n",
    "    tr_res.append([x[0] for x in tr_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "dd076b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = [normalize_text(x[0]) for x in corpus_keywords]\n",
    "vectorizer = TfidfVectorizer(stop_words=stop, smooth_idf=True, use_idf=True)\n",
    "vectorizer.fit_transform(corpus_tfidf)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "tfidf_res = [get_keywords_tfidf(text) for text in corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32936a6d",
   "metadata": {},
   "source": [
    "### задание 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089611d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e76978ac8e4b3db5e312dee8cc7116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 20:44:30 INFO: Downloading default packages for language: ru (Russian)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9f6900bdf2417aba3d49df1d66530d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.3.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 20:49:00 INFO: Finished downloading models and saved to /Users/veronicasmilga/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4f6a6",
   "metadata": {},
   "source": [
    "Функция получения списка нужных нам шаблонов с помощью stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "95e28867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_templates(corpus_keywords):\n",
    "    keywords = [x[1] for x in corpus_keywords]\n",
    "    templates = []\n",
    "    nlp = stanza.Pipeline(\"ru\")\n",
    "    for k in keywords:\n",
    "        for word in k:\n",
    "            word_tokens = [word.upos for sent in nlp(word).sentences for word in sent.words]\n",
    "            templates.append(' + '.join(word_tokens))\n",
    "    return set(templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce655f77",
   "metadata": {},
   "source": [
    "Функция фильтрации через эти шаблоны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6191d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_templates(templates, keywords):\n",
    "    filtered = []\n",
    "    for word in keywords:\n",
    "        word_tokens = [word.upos for sent in nlp(word).sentences for word in sent.words]\n",
    "        template = ' + '.join(word_tokens)\n",
    "        if template in templates:\n",
    "            filtered.append(word)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5de179",
   "metadata": {},
   "source": [
    "Ради интереса посмотрим на шаблоны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dc2a75a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 22:35:05 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2021-11-07 22:35:05 INFO: Use device: cpu\n",
      "2021-11-07 22:35:05 INFO: Loading: tokenize\n",
      "2021-11-07 22:35:05 INFO: Loading: pos\n",
      "2021-11-07 22:35:06 INFO: Loading: lemma\n",
      "2021-11-07 22:35:07 INFO: Loading: depparse\n",
      "2021-11-07 22:35:07 INFO: Loading: ner\n",
      "2021-11-07 22:35:09 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADJ + NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN + NOUN',\n",
       " 'NOUN + PROPN',\n",
       " 'NOUN + PUNCT + NOUN',\n",
       " 'PROPN',\n",
       " 'PROPN + PROPN',\n",
       " 'VERB',\n",
       " 'VERB + NOUN'}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates = get_templates(corpus_keywords)\n",
    "templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aef501",
   "metadata": {},
   "source": [
    "Получилось вполне логично, ура. Единственный странный момент, NOUN + PUNCT + NOUN, это Сан-Франциско. Отфильтруем полученные автоматически теги с помошью шаблонов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "87516269",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_res_filtered = [filter_by_templates(templates, x) for x in rake_res]\n",
    "tr_res_filtered = [filter_by_templates(templates, x) for x in tr_res]\n",
    "tfidf_res_filtered = [filter_by_templates(templates, x) for x in tfidf_res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabd38e",
   "metadata": {},
   "source": [
    "### задание 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7ad3c",
   "metadata": {},
   "source": [
    "Функция получения метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3683d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_doc(true, pred):\n",
    "    pr_total = 0\n",
    "    rec_total = 0\n",
    "    f_total = 0\n",
    "    for i in range(len(true)):\n",
    "        intersection = 0\n",
    "        for word in true[i]:\n",
    "            if word in pred[i]:\n",
    "                intersection += 1\n",
    "        precision = intersection / len(pred[i])\n",
    "        pr_total += intersection / len(pred[i])\n",
    "        recall = intersection / len(true[i])\n",
    "        rec_total += intersection / len(true[i])\n",
    "        f_total += 2 * (precision * recall) / (precision + recall)\n",
    "    precision = pr_total / len(true)\n",
    "    recall = rec_total / len(true)\n",
    "    fscore = f_total / len(true)\n",
    "    return precision, recall, fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3506c",
   "metadata": {},
   "source": [
    "Функция красивого вывода метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6dfac04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beautiful_metrics(no_filter, filtered, algo):\n",
    "    before = get_metrics_doc([x[1] for x in corpus_keywords], no_filter)\n",
    "    after = get_metrics_doc([x[1] for x in corpus_keywords], filtered)\n",
    "    return (f'''Алгоритм: {algo} \\nМетрики до фильтрации \\nprecision: {\"%.4f\" % before[0]} \\\n",
    "recall: {\"%.4f\" % before[1]} fscore: {\"%.4f\" % before[2]} \\nМетрики после фильтрации \n",
    "precision: {\"%.4f\" % after[0]} recall: {\"%.4f\" % after[1]} fscore: {\"%.4f\" % after[2]} \n",
    "Улучшение \\nprecision: {\"%.4f\" % (after[0]-before[0])} recall: {\"%.4f\" % (after[1]-before[1])} \\\n",
    "fscore: {\"%.4f\" % (after[2]-before[2])}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6a6c4ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Алгоритм: RAKE \n",
      "Метрики до фильтрации \n",
      "precision: 0.3175 recall: 0.2705 fscore: 0.2670 \n",
      "Метрики после фильтрации \n",
      "precision: 0.3533 recall: 0.2705 fscore: 0.2867 \n",
      "Улучшение \n",
      "precision: 0.0358 recall: 0.0000 fscore: 0.0198\n",
      "\n",
      "Алгоритм: TextRank \n",
      "Метрики до фильтрации \n",
      "precision: 0.1376 recall: 0.4593 fscore: 0.1914 \n",
      "Метрики после фильтрации \n",
      "precision: 0.1885 recall: 0.4593 fscore: 0.2403 \n",
      "Улучшение \n",
      "precision: 0.0509 recall: 0.0000 fscore: 0.0490\n",
      "\n",
      "Алгоритм: tf*idf \n",
      "Метрики до фильтрации \n",
      "precision: 0.2200 recall: 0.4295 fscore: 0.2885 \n",
      "Метрики после фильтрации \n",
      "precision: 0.2610 recall: 0.4295 fscore: 0.3225 \n",
      "Улучшение \n",
      "precision: 0.0410 recall: 0.0000 fscore: 0.0340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(beautiful_metrics(rake_res, rake_res_filtered, \"RAKE\"))\n",
    "print(beautiful_metrics(tr_res, tr_res_filtered, \"TextRank\"))\n",
    "print(beautiful_metrics(tfidf_res, tfidf_res_filtered, \"tf*idf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee156a",
   "metadata": {},
   "source": [
    "### задание 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9519537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementswise_join(l1, l2, l3):\n",
    "    result = [x + y + z for x, y, z in zip(l1, l2, l3)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "efbe9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aut_keywords = elementswise_join(rake_res_filtered, tr_res_filtered, tfidf_res_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "62655a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keywords = [x[1] for x in corpus_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "c5f6a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative = []\n",
    "for i in range(len(my_keywords)):\n",
    "    for word in my_keywords[i]:\n",
    "        if word not in aut_keywords[i]:\n",
    "            false_negative.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "310c8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive = []\n",
    "for i in range(len(my_keywords)):\n",
    "    for word in aut_keywords[i]:\n",
    "        if word not in my_keywords[i]:\n",
    "            false_positive.append(word)\n",
    "false_positive = set(false_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c854c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова, не найденные автоматически: пользоваться туалет, космос, возвращение, запрет, космический корабль, crew dragon, народный медицина, лечение, проблема, тренировка, забыть, запоминание, проблема, ркн, суд, rubrain, переезд, кремниевый долина, исследование\n"
     ]
    }
   ],
   "source": [
    "print(f\"Слова, не найденные автоматически:\", ', '.join(false_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2cf1c229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лишние найденные автоматически слова: находиться, основа лекарство, опыт, apple, капитализация, рубль, процесс, музей, название, узел, факт, искать факт, доступность, вид фотография, экипаж crew, хотеться, камера, представительство, турист, экскурсия музей, день рождение, использование, работа, калифорния, сок, твиттер, it, место, доработать, жизнь, понять, связанный, протокол, заметка, научный название, oracle, выехать, пройти, эффект, специалист, дело, многий, штат, говорить, работать, группа, слово, кустарник, изучение, часть, иметь, открывать, продаваться, исполнительный производство, час, город, взыскание, открыть, эмиграция, план город, мкс земля, обновление, деталь, маск, район, экипаж, онлайн, купертино, земля, элемент, улица, стоить, удалённый работа, ноябрь, inspiration, сотрудник, млн, мусор, капсула, проходить, срабатывать, стратегия, собирать, офис, информация, сожаление, twitter, прошлый год, храниться, использовать, чикаго, возбудить дело, платить, получить, проблема, знание, компания, дата, часы, млн рубль, зависимость, потенциал, собираться, время, внимание, dragon, полагаться, техас, начинать, основатель tesla, упустить, составлять, франциско, спальня, музейный экспонат, основа, способствовать, сумма, наса spacex, плата жильё, средство, молимау, участник, м, самасоня, человек, большой усилие, риск, соучредитель, посадка, множество, дом, дать, продавать, урина, возбудить, мкс, рак, центр, начало, частность, россия, сан, тысяча, телефон, гаджет, наличие, совет, туалет время, связать возможность, адрес, хранить, сахина бойдас, идти, десятка, астронавт, год, отход, начинаться, california, многое, столица, многий отношение, млрд, макартур, нью-йорк, полёт, подоходный налог, технология, улучшение, половина, число, необходимость, налог, переехать, жить, квартира, google, производство, стать, зарплата, инвестор, пытаться, помогать, facebook, последний год, исследование, crew, деньга\n"
     ]
    }
   ],
   "source": [
    "print(f\"Лишние найденные автоматически слова:\", ', '.join(false_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "074cb4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(false_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf87a66",
   "metadata": {},
   "source": [
    "### Итого:\n",
    "Наибольшая полнота наблюдалась у алгоритма TextRank, вероятно, засчет наибольшего количества сделанных предсказаний (на это указывает его низкая точность). Если совместить предсказания всех алгоритмов, окажется, что нам удалось найти практически бОльшую часть размеченных вручную ключевых слов (35 из 54). Однако, в то же время, даже с использованием шаблонов получилось очень много мусора (189 лишних слов). <br><br>Слова, которые не нашлись, это прежде всего слова, которые были важны для понимания статьи / связаны с ее тематикой, но встретились в статье совсем мало раз. Я смогла улучшить качество, добавив предобработку, хотя это не требовалось в задании. Предобработка помогла алгоритмам заметить те слова, которые встречались в тексте много раз, но в разных формах. <br><br>Большое количество найденного мусора связано с большим количеством шаблонов из-за разнородности проставленных мной тегов. Возможно, стоит при проставлении ограничить теги шаблонами ADJ+NOUN, NOUN, PROPN, тогда будет легче автоматически извлечь нужное. Кроме того, программа часто извлекала синонимы / контекстные синонимы тегов, которые также встречались в тексте, но ставить которые было бы нецелесообразно (получилоь бы много тегов с одинаковым значением). Может быть, можно было бы подключить БЕРТ, чтобы отсеивать очень близкие семантически слова."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
